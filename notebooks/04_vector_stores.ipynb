{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Vector Databases\n",
    "\n",
    "**Objective**: Build a retrieval system that efficiently searches for relevant document chunks.\n",
    "\n",
    "**Topics**:\n",
    "- Sparse vs. dense retrieval methods\n",
    "- Hybrid search methods (e.g., combining BM25 with dense retrieval)\n",
    "- Overview of vector databases: Milvus, Faiss, Qdrant\n",
    "\n",
    "**Practical Task**: Set up a vector database and implement a retrieval method.\n",
    "\n",
    "**Resources**:\n",
    "- What is a vector database\n",
    "- Choosing a vector database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vector Store from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felipecisternas/Desktop/uc/practicos-rag/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/Users/felipecisternas/Desktop/uc/practicos-rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import  Any, cast\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.bridge.pydantic import Field\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import TextNode, BaseNode\n",
    "from llama_index.core.vector_stores import MetadataFilters, VectorStoreQuery, VectorStoreQueryResult\n",
    "from llama_index.core.vector_stores.types import BasePydanticVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in some documents, and parse them into `Node` objects - chunks that are ready to be inserted into a vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse into Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceSplitter(chunk_size=256)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings for each Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 19:48:06,966 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en\n",
      "2025-11-07 19:48:12,348 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    }
   ],
   "source": [
    "embed_model  = HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Simple In-Memory Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our in-memory vector store. We'll store Nodes within a simple Python dictionary. We'll start off implementing embedding search, and add metadata filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Interface\n",
    "We'll first define the interface for building a vector store. It contains the following items:\n",
    "    - get\n",
    "    - add\n",
    "    - delete\n",
    "    - query\n",
    "    - persist (which we will not implement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVectorStore(BasePydanticVectorStore):\n",
    "    \"\"\"Simple custom Vector Store.\n",
    "\n",
    "    Stores documents in a simple in-memory dict.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    stores_text: bool = True\n",
    "\n",
    "    def client(self) -> Any:\n",
    "        \"\"\"Get client.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def get(self, text_id: str) -> list[float]:\n",
    "        \"\"\"Get embedding.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        nodes: list[BaseNode],\n",
    "    ) -> list[str]:\n",
    "        \"\"\"Add nodes to index.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Delete nodes using with ref_doc_id.\n",
    "\n",
    "        Args:\n",
    "            ref_doc_id (str): The doc_id of the document to delete.\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def query(\n",
    "        self,\n",
    "        query: VectorStoreQuery,\n",
    "        **kwargs: Any,\n",
    "    ) -> VectorStoreQueryResult:\n",
    "        \"\"\"Get nodes for response.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def persist(self, persist_path, fs=None) -> None:\n",
    "        \"\"\"Persist the SimpleVectorStore to a directory.\n",
    "\n",
    "        NOTE: we are not implementing this for now.\n",
    "\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "At a high-level, we subclass our base `VectorStore` abstraction. There's no inherent reason to do this if you're just building a vector store from scratch. We do it because it makes it easy to plug into our downstream abstractions later.\n",
    "\n",
    "Let's look at some of the classes defined here.\n",
    "\n",
    "`BaseNode` is simply the parent class of our core Node modules. Each Node represents a text chunk + associated metadata.\n",
    "We also use some lower-level constructs, for instance our `VectorStoreQuery` and `VectorStoreQueryResult`. These are just lightweight dataclass containers to represent queries and results. We look at the dataclass fields below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining add, get, and delete\n",
    "We add some basic capabilities to add, get, and delete from a vector store.\n",
    "\n",
    "The implementation is very simple (everything is just stored in a python dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore2(BaseVectorStore):\n",
    "    \"\"\"VectorStore2 (add/get/delete implemented).\"\"\"\n",
    "\n",
    "    stores_text: bool = True\n",
    "    node_dict: dict[str, BaseNode] = Field(default_factory=dict)\n",
    "\n",
    "    def get(self, text_id: str) -> list[float]:\n",
    "        \"\"\"Get embedding.\"\"\"\n",
    "        return self.node_dict[text_id]\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        nodes: list[BaseNode],\n",
    "    ) -> list[str]:\n",
    "        \"\"\"Add nodes to index.\"\"\"\n",
    "        for node in nodes:\n",
    "            self.node_dict[node.node_id] = node\n",
    "\n",
    "    def delete(self, node_id: str, **delete_kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Delete nodes using with node_id.\n",
    "\n",
    "        Args:\n",
    "            node_id: str\n",
    "\n",
    "        \"\"\"\n",
    "        del self.node_dict[node_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_node = TextNode(id_=\"id1\", text=\"hello world\")\n",
    "test_node2 = TextNode(id_=\"id2\", text=\"foo bar\")\n",
    "test_nodes = [test_node, test_node2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore2()\n",
    "\n",
    "vector_store.add(test_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: id1\n",
      "Text: hello world\n"
     ]
    }
   ],
   "source": [
    "node = vector_store.get(\"id1\")\n",
    "print(str(node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a Defining query (semantic search)\n",
    "We implement a basic version of top-k semantic search. This simply iterates through all document embeddings, and compute cosine-similarity with the query embedding. The top-k documents by cosine similarity are returned.\n",
    "\n",
    "Cosine similarity: $\\dfrac{\\vec{d}\\vec{q}}{|\\vec{d}||\\vec{q}|}$ for every document, query embedding pair $\\vec{d}$, $\\vec{p}$.\n",
    "\n",
    "NOTE: The top-k value is contained in the VectorStoreQuery container.\n",
    "\n",
    "NOTE: Similar to the above, we define another subclass just so we don't have to reimplement the above functions (not because this is actually good code practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_embeddings(\n",
    "    query_embedding: list[float],\n",
    "    doc_embeddings: list[list[float]],\n",
    "    doc_ids: list[str],\n",
    "    similarity_top_k: int = 5,\n",
    ") -> tuple[list[float], list]:\n",
    "    \"\"\"Get top nodes by similarity to the query.\"\"\"\n",
    "    # dimensions: D\n",
    "    q_embed_np = np.array(query_embedding)\n",
    "    # dimensions: N x D\n",
    "    d_embed_np = np.array(doc_embeddings)\n",
    "    # dimensions: N\n",
    "    d_product_arr = np.dot(d_embed_np, q_embed_np)\n",
    "    # dimensions: N\n",
    "    norm_arr = np.linalg.norm(q_embed_np) * np.linalg.norm(\n",
    "        d_embed_np, axis=1, keepdims=False\n",
    "    )\n",
    "    # dimensions: N\n",
    "    cos_sim_arr = d_product_arr / norm_arr\n",
    "\n",
    "    # now we have the N cosine similarities for each document\n",
    "    # sort by top k cosine similarity, and return ids\n",
    "    tups = [(cos_sim_arr[i], doc_ids[i]) for i in range(len(doc_ids))]\n",
    "    sorted_tups = sorted(tups, key=lambda t: t[0], reverse=True)\n",
    "\n",
    "    sorted_tups = sorted_tups[:similarity_top_k]\n",
    "\n",
    "    result_similarities = [s for s, _ in sorted_tups]\n",
    "    result_ids = [n for _, n in sorted_tups]\n",
    "    return result_similarities, result_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore3A(VectorStore2):\n",
    "    \"\"\"Implements semantic/dense search.\"\"\"\n",
    "\n",
    "    def query(\n",
    "        self,\n",
    "        query: VectorStoreQuery,\n",
    "        **kwargs: Any,\n",
    "    ) -> VectorStoreQueryResult:\n",
    "        \"\"\"Get nodes for response.\"\"\"\n",
    "\n",
    "        query_embedding = cast(list[float], query.query_embedding)\n",
    "        doc_embeddings = [n.embedding for n in self.node_dict.values()]\n",
    "        doc_ids = [n.node_id for n in self.node_dict.values()]\n",
    "\n",
    "        similarities, node_ids = get_top_k_embeddings(\n",
    "            query_embedding,\n",
    "            doc_embeddings,\n",
    "            doc_ids,\n",
    "            similarity_top_k=query.similarity_top_k,\n",
    "        )\n",
    "        result_nodes = [self.node_dict[node_id] for node_id in node_ids]\n",
    "\n",
    "        return VectorStoreQueryResult(\n",
    "            nodes=result_nodes, similarities=similarities, ids=node_ids\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b. Supporting Metadata Filtering\n",
    "The next extension is adding metadata filter support. This means that we will first filter the candidate set with documents that pass the metadata filters, and then perform semantic querying.\n",
    "\n",
    "For simplicity we use metadata filters for exact matching with an AND condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nodes(nodes: list[BaseNode], filters: MetadataFilters):\n",
    "    filtered_nodes = []\n",
    "    for node in nodes:\n",
    "        matches = True\n",
    "        for f in filters.filters:\n",
    "            if f.key not in node.metadata:\n",
    "                matches = False\n",
    "                continue\n",
    "            if f.value != node.metadata[f.key]:\n",
    "                matches = False\n",
    "                continue\n",
    "        if matches:\n",
    "            filtered_nodes.append(node)\n",
    "    return filtered_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add filter_nodes as a first-pass over the nodes before running semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_search(query: VectorStoreQuery, nodes: list[BaseNode]):\n",
    "    \"\"\"Dense search.\"\"\"\n",
    "    query_embedding = cast(list[float], query.query_embedding)\n",
    "    doc_embeddings = [n.embedding for n in nodes]\n",
    "    doc_ids = [n.node_id for n in nodes]\n",
    "    return get_top_k_embeddings(\n",
    "        query_embedding,\n",
    "        doc_embeddings,\n",
    "        doc_ids,\n",
    "        similarity_top_k=query.similarity_top_k,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore3B(VectorStore2):\n",
    "    \"\"\"Implements Metadata Filtering.\"\"\"\n",
    "\n",
    "    def query(\n",
    "        self,\n",
    "        query: VectorStoreQuery,\n",
    "        **kwargs: Any,\n",
    "    ) -> VectorStoreQueryResult:\n",
    "        \"\"\"Get nodes for response.\"\"\"\n",
    "        # 1. First filter by metadata\n",
    "        nodes = self.node_dict.values()\n",
    "        if query.filters is not None:\n",
    "            nodes = filter_nodes(nodes, query.filters)\n",
    "        if len(nodes) == 0:\n",
    "            result_nodes = []\n",
    "            similarities = []\n",
    "            node_ids = []\n",
    "        else:\n",
    "            # 2. Then perform semantic search\n",
    "            similarities, node_ids = dense_search(query, nodes)\n",
    "            result_nodes = [self.node_dict[node_id] for node_id in node_ids]\n",
    "        return VectorStoreQueryResult(\n",
    "            nodes=result_nodes, similarities=similarities, ids=node_ids\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Data into our Vector Store\n",
    "Let's load our text chunks into the vector store, and run it on different types of queries: dense search, w/ metadata filters, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore3B()\n",
    "# load data into the vector stores\n",
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an example question and embed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.057459719479084015, 0.0006168404361233115, 0.007399792782962322, -0.02610773779451847, 0.010598315857350826, 0.00798750203102827, 0.06677503883838654, 0.03960995748639107, 0.0025795777328312397, -0.00429012905806303, -0.051723357290029526, -0.036634426563978195, 0.01641755737364292, 0.020253067836165428, 0.0051087080501019955, 0.003774326993152499, 0.032964076846838, 0.032009292393922806, -0.008439945057034492, 0.019312605261802673, -0.004506593104451895, 0.03124888800084591, -0.024012679234147072, -0.020971687510609627, 0.00496076513081789, -0.0026956028304994106, 0.017490491271018982, 0.002253093058243394, -0.017213137820363045, -0.19273661077022552, -0.04692847281694412, -0.03517276048660278, -0.02127939835190773, -0.007309691980481148, -0.02094404771924019, -0.008686757646501064, -0.02881697379052639, 0.031263578683137894, -0.004988396540284157, -0.003774901619181037, 0.04736080393195152, 0.017691638320684433, -0.008748934604227543, -0.009141998365521431, -0.01443556509912014, -0.03179143741726875, 0.014844898134469986, -0.014371486380696297, 0.011276040226221085, -0.03396739438176155, 0.02256646752357483, -0.06226884201169014, 0.01112303789705038, 0.040709856897592545, 0.006193358451128006, -0.019459795206785202, 0.04752477630972862, -0.017023222520947456, 0.03172088414430618, 0.03288189694285393, 0.03563746064901352, 0.004961818922311068, -0.22939303517341614, 0.05001296475529671, 0.03738638013601303, 0.017077848315238953, -0.060554128140211105, -0.007469156291335821, 0.04383736848831177, 0.05219804123044014, -0.04051164537668228, 0.010734785348176956, -0.002665082225576043, 0.0346045047044754, 0.011445983313024044, -0.0006426668260246515, -0.003522442886605859, -0.022404920309782028, -0.030261030420660973, 0.019131053239107132, 0.0026318139862269163, 0.0030924491584300995, -0.00868062674999237, -0.004652420990169048, -0.035332098603248596, -0.02263445034623146, 0.008173711597919464, -0.01849713735282421, 0.05894792079925537, -0.027575714513659477, -0.00394861726090312, -0.014733308926224709, 0.015612207353115082, 0.004900637548416853, -0.016957513988018036, -0.0298172514885664, 0.022692879661917686, 0.001968824304640293, -0.05924457684159279, 0.5568937659263611, 0.014852047897875309, 0.00768244219943881, -0.0003968370147049427, -0.043283917009830475, -0.0009033113019540906, -0.00047670555068179965, -0.0028102847281843424, -0.03579295054078102, 0.03440878540277481, -0.01750088855624199, 0.06579441577196121, 0.02140825241804123, 0.035790979862213135, -0.05869940668344498, 0.022129947319626808, 0.011061991564929485, 0.024873562157154083, 0.03236912190914154, 0.04357122257351875, 0.01154380664229393, 0.005172893404960632, -5.4678410378983244e-05, 0.023215463384985924, -0.022813940420746803, 0.010534724220633507, -0.05636307969689369, 0.015465613454580307, 0.0316527783870697, 0.015002790838479996, 0.04551691934466362, 0.035071514546871185, -0.04583929851651192, -0.01144354697316885, 0.0050998879596591, 0.042536817491054535, 0.02628357894718647, 0.0019311201758682728, 0.04406760632991791, 0.03663931041955948, 0.016830667853355408, -0.006389941088855267, -0.027171745896339417, 0.006929400842636824, -0.08807393908500671, -0.010195172391831875, 0.07470181584358215, -0.04653770104050636, -0.03825313597917557, 0.004255255684256554, 0.02797759138047695, -0.026997584849596024, -0.024262424558401108, -0.0027872100472450256, 0.009473673067986965, 0.048838552087545395, 0.018473470583558083, 0.06538654863834381, -0.004449255298823118, 0.01015683263540268, 0.0030983800534158945, 0.027741804718971252, -0.05556008592247963, -0.04202958568930626, 0.10762950032949448, 0.020321441814303398, -0.0843367949128151, -0.034881480038166046, 0.027963951230049133, 0.005946196615695953, -0.07063161581754684, 0.013553624972701073, 0.04392727464437485, 0.017480146139860153, 0.007077404763549566, 0.04793951287865639, 0.03988019376993179, -0.03746941685676575, -0.0051566800102591515, -0.0017877500504255295, 0.012780856341123581, 0.029970834031701088, -0.05530881509184837, -0.027438417077064514, 0.027380255982279778, 0.0065740603022277355, -0.03124306909739971, -0.016395527869462967, -0.025616906583309174, 0.011955881491303444, 0.03154737502336502, -0.0755361020565033, -0.021062761545181274, -0.05147046968340874, 0.01225479319691658, -0.018709849566221237, 0.019769174978137016, -0.02266046777367592, -0.04181556776165962, -0.03319884464144707, -0.018095672130584717, -0.0180714912712574, 0.03097117692232132, -0.024407027289271355, 0.020371761173009872, -0.016694752499461174, 0.059503499418497086, 0.006635421887040138, -0.052398961037397385, 0.021864689886569977, 0.0466967336833477, -0.04731142148375511, 0.01846756413578987, 0.030451608821749687, -0.02965143322944641, 0.0274912491440773, -0.011497722938656807, 0.03049095720052719, 0.08592380583286285, -0.014730957336723804, 0.0038556032814085484, 0.03972642496228218, -0.007360913325101137, -0.05267251282930374, -0.26506251096725464, -0.004205159842967987, -0.04992307722568512, -0.04608023539185524, 0.04779163375496864, 0.005346050951629877, 0.004466724116355181, -0.05271114036440849, -0.03401686251163483, 0.010383150540292263, 0.10346557199954987, -0.011907000094652176, -0.03485244885087013, -0.004784584976732731, 0.003324117511510849, -0.016659816727042198, -0.04104219377040863, -0.024615373462438583, -0.02971380203962326, 0.025853727012872696, 0.021279117092490196, 0.04438545182347298, -0.0225178562104702, -0.045140791684389114, 0.00627316115424037, -0.04900108277797699, 0.1533016860485077, -0.06291750818490982, 0.03405473753809929, -0.08710365742444992, 0.032095279544591904, 0.0022565280087292194, 0.008197574876248837, -0.09039773792028427, 0.07180225104093552, -0.02014799974858761, -0.02002089098095894, 0.03983256593346596, -0.024962974712252617, -0.02909989282488823, -0.014416358433663845, 0.0013769103679805994, -0.05808037519454956, -0.03395568206906319, -0.05655746906995773, -0.05444037914276123, -0.02733142115175724, 0.016929492354393005, -0.017358047887682915, 0.006731732282787561, 0.005832977592945099, 0.006197105161845684, 0.008196965791285038, 0.05873089283704758, 0.0012966339709237218, -0.05289774760603905, -0.05567115172743797, 0.059311285614967346, -0.08881248533725739, -0.01713274046778679, -0.007047215476632118, -0.0030120317824184895, 0.04013107717037201, -0.03332123905420303, 0.035173434764146805, 0.006589079275727272, 0.0017237311694771051, -0.023320075124502182, 0.006192929111421108, -0.032996539026498795, -0.03481079638004303, 0.08181259036064148, 0.019100246950984, -0.0030929776839911938, 0.03229944780468941, 0.006326558068394661, -0.01688062585890293, -0.013847123831510544, -0.022015366703271866, -0.015142230316996574, 0.019616037607192993, -0.005917531903833151, 0.033463846892118454, 0.0038561346009373665, -0.023139311000704765, -0.008749743923544884, 0.006105163134634495, -0.025310466066002846, 0.011858776211738586, 0.018025174736976624, -0.014152341522276402, -0.011295868083834648, -0.0023381710052490234, -0.028960108757019043, -0.0025235414505004883, 0.02221144363284111, -0.27014416456222534, 0.020244445651769638, -0.03726724907755852, 0.023057498037815094, 0.006620155647397041, -0.005738267209380865, 0.05292988196015358, -0.02519232966005802, 0.033661648631095886, 0.017388977110385895, 0.003910520579665899, 0.04383259639143944, 0.016763465479016304, 0.0313408188521862, 0.003478233003988862, 0.02327924594283104, 0.03731159493327141, -0.01488049142062664, -0.021189991384744644, 0.010380102321505547, 0.0011236966820433736, 0.0351155549287796, 0.17353887856006622, 0.009348683059215546, 0.010357178747653961, 0.028976988047361374, -0.006824260111898184, 0.017707111313939095, 0.014328143559396267, 0.006178721319884062, 0.04560866206884384, -0.012570991180837154, 0.0733330175280571, -0.032031238079071045, 0.02933594584465027, 0.02715870551764965, -0.0029565284494310617, 0.052192676812410355, 0.03671596944332123, 0.00256909872405231, -0.0026360806077718735, -0.023730378597974777, 0.05152146890759468, -0.027746913954615593, 0.05118774622678757, -0.02967192232608795, -0.042937520891427994, -0.05463382229208946, -0.004475359804928303, 0.014210390858352184, -0.03161514177918434, -0.008067318238317966, -0.0030675530433654785, -0.02350553870201111, 0.049677975475788116, 0.04547760635614395, 0.008374730125069618, 0.014891320839524269, -0.00041669662459753454, -0.013337049633264542, 0.021680042147636414, -0.007706380449235439, -0.03791989013552666, 0.045142631977796555, -0.006717766169458628]\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Can you tell me about the key concepts for safety finetuning\"\n",
    "query_embedding = embed_model.get_query_embedding(query_str)\n",
    "print(query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the vector store with dense search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------\n",
      "[Node ID 45ab3492-2272-4969-a3cc-54aa1a368822] Similarity: 0.8711294993852078\n",
      "\n",
      "total_pages: 77\n",
      "file_path: ./data/llama2.pdf\n",
      "source: 23\n",
      "\n",
      "Benchmarks give a summary view of model capabilities and behaviors that allow us to understand general\n",
      "patterns in the model, but they do not provide a fully comprehensive view of the impact the model may have\n",
      "on people or real-world outcomes; that would require study of end-to-end product deployments. Further\n",
      "testing and mitigation should be done to understand bias and other social issues for the specific context\n",
      "in which a system may be deployed. For this, it may be necessary to test beyond the groups available in\n",
      "the BOLD dataset (race, religion, and gender). As LLMs are integrated and deployed, we look forward to\n",
      "continuing research that will amplify their potential for positive impact on these important social issues.\n",
      "4.2\n",
      "Safety Fine-Tuning\n",
      "In this section, we describe our approach to safety fine-tuning, including safety categories, annotation\n",
      "guidelines, and the techniques we use to mitigate safety risks. We employ a process similar to the general\n",
      "fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.\n",
      "----------------\n",
      "\n",
      "\n",
      "\n",
      "----------------\n",
      "[Node ID 1bee715a-ba1b-4822-bcfa-e02b6582014a] Similarity: 0.8678353893326725\n",
      "\n",
      "total_pages: 77\n",
      "file_path: ./data/llama2.pdf\n",
      "source: 23\n",
      "\n",
      "Further\n",
      "testing and mitigation should be done to understand bias and other social issues for the specific context\n",
      "in which a system may be deployed. For this, it may be necessary to test beyond the groups available in\n",
      "the BOLD dataset (race, religion, and gender). As LLMs are integrated and deployed, we look forward to\n",
      "continuing research that will amplify their potential for positive impact on these important social issues.\n",
      "4.2\n",
      "Safety Fine-Tuning\n",
      "In this section, we describe our approach to safety fine-tuning, including safety categories, annotation\n",
      "guidelines, and the techniques we use to mitigate safety risks. We employ a process similar to the general\n",
      "fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.\n",
      "Specifically, we use the following techniques in safety fine-tuning:\n",
      "1. Supervised Safety Fine-Tuning: We initialize by gathering adversarial prompts and safe demonstra-\n",
      "tions that are then included in the general supervised fine-tuning process (Section 3.1).\n",
      "----------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_obj = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=2\n",
    ")\n",
    "\n",
    "query_result = vector_store.query(query_obj)\n",
    "for similarity, node in zip(query_result.similarities, query_result.nodes):\n",
    "    print(\n",
    "        \"\\n----------------\\n\"\n",
    "        f\"[Node ID {node.node_id}] Similarity: {similarity}\\n\\n\"\n",
    "        f\"{node.get_content(metadata_mode='all')}\"\n",
    "        \"\\n----------------\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the vector store with dense search + Metadata Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------\n",
      "[Node ID 459aba14-1d81-4948-b581-4605636f2520] Similarity: 0.8589715396624741\n",
      "\n",
      "total_pages: 77\n",
      "file_path: ./data/llama2.pdf\n",
      "source: 24\n",
      "\n",
      "We then define best practices for safe and helpful model responses: the model should first address immediate\n",
      "safety concerns if applicable, then address the prompt by explaining the potential risks to the user, and finally\n",
      "provide additional information if possible. We also ask the annotators to avoid negative user experience\n",
      "categories (see Appendix A.5.2). The guidelines are meant to be a general guide for the model and are\n",
      "iteratively refined and revised to include newly identified risks.\n",
      "4.2.2\n",
      "Safety Supervised Fine-Tuning\n",
      "In accordance with the established guidelines from Section 4.2.1, we gather prompts and demonstrations\n",
      "of safe model responses from trained annotators, and use the data for supervised fine-tuning in the same\n",
      "manner as described in Section 3.1. An example can be found in Table 5.\n",
      "The annotators are instructed to initially come up with prompts that they think could potentially induce\n",
      "the model to exhibit unsafe behavior, i.e., perform red teaming, as defined by the guidelines. Subsequently,\n",
      "annotators are tasked with crafting a safe and helpful response that the model should produce.\n",
      "----------------\n",
      "\n",
      "\n",
      "\n",
      "----------------\n",
      "[Node ID d76c2e16-e262-44f5-becf-828174beb780] Similarity: 0.8495982022359574\n",
      "\n",
      "total_pages: 77\n",
      "file_path: ./data/llama2.pdf\n",
      "source: 24\n",
      "\n",
      "advice). The attack vectors explored consist of psychological manipulation (e.g., authority manipulation),\n",
      "logic manipulation (e.g., false premises), syntactic manipulation (e.g., misspelling), semantic manipulation\n",
      "(e.g., metaphor), perspective manipulation (e.g., role playing), non-English languages, and others.\n",
      "We then define best practices for safe and helpful model responses: the model should first address immediate\n",
      "safety concerns if applicable, then address the prompt by explaining the potential risks to the user, and finally\n",
      "provide additional information if possible. We also ask the annotators to avoid negative user experience\n",
      "categories (see Appendix A.5.2). The guidelines are meant to be a general guide for the model and are\n",
      "iteratively refined and revised to include newly identified risks.\n",
      "4.2.2\n",
      "Safety Supervised Fine-Tuning\n",
      "In accordance with the established guidelines from Section 4.2.1, we gather prompts and demonstrations\n",
      "of safe model responses from trained annotators, and use the data for supervised fine-tuning in the same\n",
      "manner as described in Section 3.1.\n",
      "----------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gp/286d79g11513f730wt0d61kc0000gq/T/ipykernel_34123/4088133968.py:6: DeprecationWarning: Call to deprecated class method from_dict. (`from_dict()` is deprecated. Please use `MetadataFilters(filters=.., condition='and')` directly instead.)\n",
      "  filters = MetadataFilters.from_dict({\"source\": \"24\"})\n"
     ]
    }
   ],
   "source": [
    "# filters = MetadataFilters(\n",
    "#     filters=[\n",
    "#         ExactMatchFilter(key=\"page\", value=3)\n",
    "#     ]\n",
    "# )\n",
    "filters = MetadataFilters.from_dict({\"source\": \"24\"})\n",
    "\n",
    "query_obj = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=2, filters=filters\n",
    ")\n",
    "\n",
    "query_result = vector_store.query(query_obj)\n",
    "for similarity, node in zip(query_result.similarities, query_result.nodes):\n",
    "    print(\n",
    "        \"\\n----------------\\n\"\n",
    "        f\"[Node ID {node.node_id}] Similarity: {similarity}\\n\\n\"\n",
    "        f\"{node.get_content(metadata_mode='all')}\"\n",
    "        \"\\n----------------\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a RAG System with the Vector Store\n",
    "Now that we've built the RAG system, it's time to plug it into our downstream system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"google/gemma-3-270m\", \n",
    "    tokenizer_name=\"google/gemma-3-270m\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why is life worth having? How do we define it? How can we be sure that it is really worth having?\n",
      "\n",
      "These are just a few questions the world of quantum physics has been asking for the past 25 years, and their answers are still getting better and better each and every day. Quantum physics is at the forefront of a wide array of scientific and technological innovations that impact many of our daily lives. If quantum physics is the answer to any of these questions, it will be a monumental step forward for humanity.\n",
      "\n",
      "<h2><strong>15. Are you ever going to understand what you think?</strong></h2>\n",
      "\n",
      "When we are asking the question, “What is life worth?” we are never truly getting to the point of actually understanding what life is worth. The world is full of complex things – things that we are never going to understand.\n",
      "\n",
      "This brings us back to our original question. What is life worth? The first question is not going to be answered by us. The only way you will ever know what life is worth is by asking the right question – <em>what is it worth?</em> This is why people ask a lot of questions, because all the answers are always going to be different.\n",
      "\n",
      "The answer we are trying to get from what we\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"What is the meaning of life?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Can you tell me about the key concepts for safety finetuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2\n",
      "\n",
      "2. Unsupervised Safety Fine-Tuning:\n",
      "We use unsupervised safety fine-tuning to build more adversarial datasets with more context and\n",
      "data about the topic under investigation.\n",
      "Total Pages: 77\n",
      "file_path: ./data/llama2.pdf\n",
      "source: 23\n",
      "\n",
      "The resulting\n",
      "dataset is then further fine-tuned using supervised and unsupervised fine-tuning methods to reduce the\n",
      "variation in model performance. This is done by training a model with training data from which it can\n",
      "easily re-fit with new data from which it is not. It also allows the model to scale as the model is applied to\n",
      "new datasets, such as human speech annotations. The process differs from that described in Section 3.2.\n",
      "4.2\n",
      "Safety Fine-Tuning\n",
      "In this section, we describe our approach to safety fine-tuning, including safety categories, annotation\n",
      "guidelines, and the techniques we use to mitigate safety risks. We employ a process similar to the general\n",
      "fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.\n",
      "Specifically, we use the following techniques in safety fine-tuning:\n",
      "1. Supervised Safety Fine-Tuning: We initialize\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "That's it! We've built a simple in-memory vector store that supports very simple inserts, gets, deletes, and supports dense search and metadata filtering. This can then be plugged into the rest of LlamaIndex abstractions.\n",
    "\n",
    "It doesn't support sparse search yet and is obviously not meant to be used in any sort of actual app. But this should expose some of what's going on under the hood!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vector Store Using Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "file_path = (\n",
    "   \"data/visual_instruction_tunning.pdf\"\n",
    ")\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Visual Instruction Tuning\\nHaotian Liu1∗, Chunyuan Li2∗, Qingyang Wu3, Yong Jae Lee1\\n1University of Wisconsin–Madison 2Microsoft Research 3Columbia University\\nhttps://llava-vl.github.io\\nAbstract\\nInstruction tuning large language models (LLMs) using machine-generated\\ninstruction-following data has been shown to improve zero-shot capabilities on\\nnew tasks, but the idea is less explored in the multimodal field. We present the\\nfirst attempt to use language-only GPT-4 to generate multimodal language-image\\ninstruction-following data. By instruction tuning on such generated data, we in-\\ntroduce LLaV A:Large Language and Vision Assistant, an end-to-end trained\\nlarge multimodal model that connects a vision encoder and an LLM for general-\\npurpose visual and language understanding. To facilitate future research on visual\\ninstruction following, we construct two evaluation benchmarks with diverse and\\nchallenging application-oriented tasks. Our experiments show that LLaV A demon-\\nstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors\\nof multimodal GPT-4 on unseen images/instructions, and yields a 85.1% rela-\\ntive score compared with GPT-4 on a synthetic multimodal instruction-following\\ndataset. When fine-tuned on Science QA, the synergy of LLaV A and GPT-4\\nachieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated\\nvisual instruction tuning data, our model, and code publicly available.\\n1 Introduction\\nHumans interact with the world through many channels such as vision and language, as each\\nindividual channel has a unique advantage in representing and communicating certain concepts, and\\nthus facilitates a better understanding of the world. One of the core aspirations in artificial intelligence\\nis to develop a general-purpose assistant that can effectively follow multi-modal vision-and-language\\ninstructions, aligned with human intent to complete various real-world tasks in the wild [4, 27, 26].\\nTo this end, the community has witnessed an emergent interest in developing language-augmented\\nfoundation vision models [ 27, 16], with strong capabilities in open-world visual understanding\\nsuch as classification [ 40, 21, 57, 54, 39], detection [ 29, 62, 33], segmentation [ 25, 63, 58] and\\ncaptioning [50, 28], as well as visual generation and editing [42, 43, 56, 15, 44, 30]. We refer readers\\nto the Computer Vision in the Wildreading list for a more up-to-date literature compilation [12]. In\\nthis line of work, each task is solved independently by one single large vision model, with the task\\ninstruction implicitly considered in the model design. Further, language is only utilized to describe\\nthe image content. While this allows language to play an important role in mapping visual signals to\\nlanguage semantics—a common channel for human communication, it leads to models that usually\\nhave a fixed interface with limited interactivity and adaptability to the user’s instructions.\\nLarge language models (LLM), on the other hand, have shown that language can play a wider\\nrole: a universal interface for a general-purpose assistant, where various task instructions can be\\nexplicitly represented in language and guide the end-to-end trained neural assistant to switch to the\\ntask of interest to solve it. For example, the recent success of ChatGPT [35] and GPT-4 [36] have\\ndemonstrated the power of aligned LLMs in following human instructions, and have stimulated\\ntremendous interest in developing open-source LLMs. Among them, LLaMA [ 49] is an open-\\nsource LLM that matches the performance of GPT-3. Alpaca [ 48], Vicuna [9], GPT-4-LLM [38]\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\\narXiv:2304.08485v2  [cs.CV]  11 Dec 2023'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 19:50:10,923 - INFO - Use pytorch device_name: mps\n",
      "2025-11-07 19:50:10,923 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.027461927384138107, 0.04477216303348541, -0.015285043977200985]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "embedded_document = embedding_model.embed_query(docs[0].page_content)\n",
    "embedded_document[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(path=\"/tmp/langchain_qdrant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete_collection(collection_name=\"demo_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"demo_collection\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"demo_collection\",\n",
    "    embedding=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['176204090b3345b88b68ce641029cb90',\n",
       " 'a54c19715ac14a22aab0f1da69167db1',\n",
       " '4469b9cee609494fb88a68b5193e1129',\n",
       " '4de77f3d5f7148c09072cb036768dd34',\n",
       " '2ce6b79d30bb4a9e99caeea4228481e9',\n",
       " 'bddf4230fe264f779e7830f3b5a8ae8a',\n",
       " '079f1905fde5434b91bc866315cd0cce',\n",
       " '8ae80c13fa05483e84a9e8fba6d5f2c8',\n",
       " '5cddc057514648abb6f7b21aaa9034f9',\n",
       " '6cfff7eb883a48adb2c18ccbbd0b35d9',\n",
       " '4966de4c485d4ed898f3075ad6a8a91f',\n",
       " 'b3afd7844f09490d9252ac71c4444f1d',\n",
       " '9644d426c3914243bc8f72bec4d404b7',\n",
       " '9617fe07f12741adb7108641bce6bc3a',\n",
       " '292c7d5cdc4749d3820b6c20f2a4c90e',\n",
       " 'c6a2ab29ba964c4c94a2d8aa7dd192a6',\n",
       " 'b666722b009b4429b965b21abeb6415a',\n",
       " '22d98c5553dc4bad99e47bbce01c51ce',\n",
       " '37542ce8c9364b9e8eb51cc2af4df218',\n",
       " '320a49502b42402aa4e525163f7e0cc2',\n",
       " '727f32fcd09749ed8e4494e039f0d6b4',\n",
       " '59709793229348f8a3687a4f75a3acc5',\n",
       " 'bd605146ca904ffeaed506679a999028',\n",
       " '25998e552ce8455e9162b8a1c8b68994',\n",
       " '44a08f65cf1c4f95bc389bc0ef34e90f',\n",
       " 'd81081a5f2394dca9a159165ba18bb76',\n",
       " 'b7f2947ff8574267ac91bef3f4ff9acb',\n",
       " 'c520f0359bb6446284c74fc908958386',\n",
       " '9c1b515339ff4f05bf5eb8d0045d13f0',\n",
       " '2187e0cab6e54b1698b0bdaf042fe831',\n",
       " 'cb72ef33ec304f3b95b0c94abe3256b1',\n",
       " '0e805e298b21454b9ea749305a88d66a',\n",
       " '5231a1540ee24e39a9538b5861ed95bd',\n",
       " '35df45af9f564651a861485832c32c6d']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Record(id='079f1905fde5434b91bc866315cd0cce', payload={'page_content': 'H v\\nImageLanguage Instruction\\nLanguage Response \\n<latexit sha1_base64=\"/KN5R7NUwEKH6XBR4DKeLzGzIrU=\">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIqMuimy4r2Ac0IUymk3bo5OHMjVhD8FfcuFDErf/hzr9x2mahrQcuHM65l3vv8RPBFVjWt1FaWl5ZXSuvVzY2t7Z3zN29topTSVmLxiKWXZ8oJnjEWsBBsG4iGQl9wTr+6Hrid+6ZVDyObmGcMDckg4gHnBLQkmceZI4f4EbuZQ6wBwDI7vLcM6tWzZoCLxK7IFVUoOmZX04/pmnIIqCCKNWzrQTcjEjgVLC84qSKJYSOyID1NI1IyJSbTa/P8bFW+jiIpa4I8FT9PZGRUKlx6OvOkMBQzXsT8T+vl0Jw6WY8SlJgEZ0tClKBIcaTKHCfS0ZBjDUhVHJ9K6ZDIgkFHVhFh2DPv7xI2qc1+7xm35xV61dFHGV0iI7QCbLRBaqjBmqiFqLoET2jV/RmPBkvxrvxMWstGcXMPvoD4/MHSKCVxg==</latexit>\\nH q\\n<latexit sha1_base64=\"4a/5KuBhqFrRimsGds8xVP6ZkkY=\">AAAB/XicbVDLSsNAFJ34rPUVHzs3g0VwVRIRdVl047KCfUAbwmQ6aYdOJnHmRqwh+CtuXCji1v9w5984bbPQ1gMXDufcy733BIngGhzn21pYXFpeWS2tldc3Nre27Z3dpo5TRVmDxiJW7YBoJrhkDeAgWDtRjESBYK1geDX2W/dMaR7LWxglzItIX/KQUwJG8u39rBuEuJ37WRfYAwBkd3nu2xWn6kyA54lbkAoqUPftr24vpmnEJFBBtO64TgJeRhRwKlhe7qaaJYQOSZ91DJUkYtrLJtfn+MgoPRzGypQEPFF/T2Qk0noUBaYzIjDQs95Y/M/rpBBeeBmXSQpM0umiMBUYYjyOAve4YhTEyBBCFTe3YjogilAwgZVNCO7sy/OkeVJ1z6ruzWmldlnEUUIH6BAdIxedoxq6RnXUQBQ9omf0it6sJ+vFerc+pq0LVjGzh/7A+vwBYcCV1g==</latexit>\\nX q\\n<latexit sha1_base64=\"I8RxJE902anMmciAczfxKVfe1PY=\">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIqMuiG5cV7APaECbTSTt0MgkzN2INwV9x40IRt/6HO//GaZuFth64cDjnXu69J0gE1+A431ZpaXllda28XtnY3NresXf3WjpOFWVNGotYdQKimeCSNYGDYJ1EMRIFgrWD0fXEb98zpXks72CcMC8iA8lDTgkYybcPsl4Q4k7uZz1gDwCQkTz37apTc6bAi8QtSBUVaPj2V68f0zRiEqggWnddJwEvIwo4FSyv9FLNEkJHZMC6hkoSMe1l0+tzfGyUPg5jZUoCnqq/JzISaT2OAtMZERjqeW8i/ud1UwgvvYzLJAUm6WxRmAoMMZ5EgftcMQpibAihiptbMR0SRSiYwComBHf+5UXSOq255zX39qxavyriKKNDdIROkIsuUB3doAZqIooe0TN6RW/Wk/VivVsfs9aSVczsoz+wPn8ASWCVxg==</latexit>\\nX a\\nLanguage Model\\nFigure 1: LLaV A network architecture.\\nFor an input image Xv, we consider the pre-trained CLIP visual encoder ViT-L/14 [ 40], which\\nprovides the visual feature Zv = g(Xv). The grid features before and after the last Transformer layer\\nare considered in our experiments. We consider a simple linear layer to connect image features into\\nthe word embedding space. Specifically, we apply a trainable projection matrix W to convert Zv into\\nlanguage embedding tokens Hv, which have the same dimensionality as the word embedding space\\nin the language model:\\nHv = W · Zv, with Zv = g(Xv) (1)\\nThus, we have a sequence of visual tokens Hv. Note that our simple projection scheme is lightweight,\\nwhich allows us to iterate data centric experiments quickly. More sophisticated schemes to con-\\nnect the image and language representations can also be considered, such as gated cross-attention\\nin Flamingo [ 2] and Q-former in BLIP-2 [ 28]. We leave exploring possibly more effective and\\nsophisticated architecture designs for LLaV A as future work.\\n4.2 Training\\nFor each image Xv, we generate multi-turn conversation data (X1\\nq, X1\\na, ··· , XT\\nq , XT\\na ), where T is\\nthe total number of turns. We organize them as a sequence, by treating all answers as the assistant’s\\nresponse, and the instruction Xt\\ninstruct at the t-th turn as:\\nXt\\ninstruct =\\n\\x1a Randomly choose [X1\\nq, Xv] or [Xv, X1\\nq], the first turn t = 1\\nXt\\nq, the remaining turns t >1 (2)\\nThis leads to the unified format for the multimodal instruction-following sequence illustrated in\\nTable 2. We perform instruction-tuning of the LLM on the prediction tokens, using its original\\nauto-regressive training objective.\\nSpecifically, for a sequence of length L, we compute the probability of the target answers Xa by:\\np(Xa|Xv, Xinstruct) =\\nLY\\ni=1\\npθ(xi|Xv, Xinstruct,<i, Xa,<i), (3)\\n4', 'metadata': {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4'}}, vector=None, shard_key=None, order_value=None),\n",
       "  Record(id='0e805e298b21454b9ea749305a88d66a', payload={'page_content': 'Context type 1: Captions\\nA group of people standing outside of a black vehicle with various luggage.\\nLuggage surrounds a vehicle in an underground parking area\\nPeople try to fit all of their luggage in an SUV .\\nThe sport utility vehicle is parked in the public garage, being packed for a trip\\nSome people with luggage near a van that is transporting it.\\nContext type 2: Boxes\\nperson: [0.681, 0.242, 0.774, 0.694], person: [0.63, 0.222, 0.686, 0.516], person: [0.444, 0.233,\\n0.487, 0.34], backpack: [0.384, 0.696, 0.485, 0.914], backpack: [0.755, 0.413, 0.846, 0.692],\\nsuitcase: [0.758, 0.413, 0.845, 0.69], suitcase: [0.1, 0.497, 0.173, 0.579], bicycle: [0.282, 0.363,\\n0.327, 0.442], car: [0.786, 0.25, 0.848, 0.322], car: [0.783, 0.27, 0.827, 0.335], car: [0.86, 0.254,\\n0.891, 0.3], car: [0.261, 0.101, 0.787, 0.626]\\nResponse type 1: conversation\\nQuestion: What type of vehicle is featured in the image?\\nAnswer: The image features a black sport utility vehicle (SUV).\\nQuestion: Where is the vehicle parked?\\nAnswer: The vehicle is parked in an underground parking area, likely in a public garage.\\nQuestion: What are the people in the image doing?\\nAnswer: The people are trying to fit all of their luggage into the SUV , likely preparing for a trip.\\nResponse type 2: detailed description\\nThe image is an underground parking area with a black sport utility vehicle (SUV) parked. There\\nare three people in the scene, with one person standing closer to the left side of the vehicle, another\\nperson in the middle, and the third person on the right side. They are all working together to pack\\ntheir luggage into the SUV for a trip.\\nIn the parking area, various luggage items are scattered around the vehicle. There are two backpacks,\\none located near the left rear wheel and the other closer to the right side of the vehicle. Additionally,\\nthere are two suitcases, one on the right side of the car and another further away near the center of\\nthe parking area. A bicycle can also be seen on the left side of the vehicle.\\nOther cars are parked around the main SUV , with one car positioned behind it and slightly to the left,\\nanother behind and slightly to the right, and the third car further behind on the right side.\\nResponse type 3: complex reasoning\\nQuestion: What challenges do these people face?\\nAnswer: In the image, a group of people is standing outside a black SUV in a parking area,\\nsurrounded by various pieces of luggage, including suitcases and backpacks. They are facing the\\nchallenge of fitting all their luggage into the black SUV . There are multiple suitcases and backpacks\\nto be packed, which suggests that the group has a significant amount of belongings to accommodate.\\nThey might have to strategize and arrange the luggage efficiently to ensure that everything fits\\nproperly into the vehicle. Additionally, they need to consider the comfort of the passengers and\\nvisibility while driving, so the placement of the luggage must not obstruct the driver’s view or make\\nthe passengers uncomfortable during the trip.\\nTable 14: One example to illustrate the instruction-following data. The top block shows the contexts\\nsuch as captions and boxes used to prompt GPT, and the bottom block shows the three types of\\nresponses. Note that the visual image is not used to prompt GPT, we only show it here as a reference.\\n23', 'metadata': {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 22, 'page_label': '23'}}, vector=None, shard_key=None, order_value=None),\n",
       "  Record(id='176204090b3345b88b68ce641029cb90', payload={'page_content': 'Visual Instruction Tuning\\nHaotian Liu1∗, Chunyuan Li2∗, Qingyang Wu3, Yong Jae Lee1\\n1University of Wisconsin–Madison 2Microsoft Research 3Columbia University\\nhttps://llava-vl.github.io\\nAbstract\\nInstruction tuning large language models (LLMs) using machine-generated\\ninstruction-following data has been shown to improve zero-shot capabilities on\\nnew tasks, but the idea is less explored in the multimodal field. We present the\\nfirst attempt to use language-only GPT-4 to generate multimodal language-image\\ninstruction-following data. By instruction tuning on such generated data, we in-\\ntroduce LLaV A:Large Language and Vision Assistant, an end-to-end trained\\nlarge multimodal model that connects a vision encoder and an LLM for general-\\npurpose visual and language understanding. To facilitate future research on visual\\ninstruction following, we construct two evaluation benchmarks with diverse and\\nchallenging application-oriented tasks. Our experiments show that LLaV A demon-\\nstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors\\nof multimodal GPT-4 on unseen images/instructions, and yields a 85.1% rela-\\ntive score compared with GPT-4 on a synthetic multimodal instruction-following\\ndataset. When fine-tuned on Science QA, the synergy of LLaV A and GPT-4\\nachieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated\\nvisual instruction tuning data, our model, and code publicly available.\\n1 Introduction\\nHumans interact with the world through many channels such as vision and language, as each\\nindividual channel has a unique advantage in representing and communicating certain concepts, and\\nthus facilitates a better understanding of the world. One of the core aspirations in artificial intelligence\\nis to develop a general-purpose assistant that can effectively follow multi-modal vision-and-language\\ninstructions, aligned with human intent to complete various real-world tasks in the wild [4, 27, 26].\\nTo this end, the community has witnessed an emergent interest in developing language-augmented\\nfoundation vision models [ 27, 16], with strong capabilities in open-world visual understanding\\nsuch as classification [ 40, 21, 57, 54, 39], detection [ 29, 62, 33], segmentation [ 25, 63, 58] and\\ncaptioning [50, 28], as well as visual generation and editing [42, 43, 56, 15, 44, 30]. We refer readers\\nto the Computer Vision in the Wildreading list for a more up-to-date literature compilation [12]. In\\nthis line of work, each task is solved independently by one single large vision model, with the task\\ninstruction implicitly considered in the model design. Further, language is only utilized to describe\\nthe image content. While this allows language to play an important role in mapping visual signals to\\nlanguage semantics—a common channel for human communication, it leads to models that usually\\nhave a fixed interface with limited interactivity and adaptability to the user’s instructions.\\nLarge language models (LLM), on the other hand, have shown that language can play a wider\\nrole: a universal interface for a general-purpose assistant, where various task instructions can be\\nexplicitly represented in language and guide the end-to-end trained neural assistant to switch to the\\ntask of interest to solve it. For example, the recent success of ChatGPT [35] and GPT-4 [36] have\\ndemonstrated the power of aligned LLMs in following human instructions, and have stimulated\\ntremendous interest in developing open-source LLMs. Among them, LLaMA [ 49] is an open-\\nsource LLM that matches the performance of GPT-3. Alpaca [ 48], Vicuna [9], GPT-4-LLM [38]\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\\narXiv:2304.08485v2  [cs.CV]  11 Dec 2023', 'metadata': {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1'}}, vector=None, shard_key=None, order_value=None)],\n",
       " '2187e0cab6e54b1698b0bdaf042fe831')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.scroll(collection_name=\"demo_collection\", limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import RetrievalMode\n",
    "\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    docs,\n",
    "    embedding=embedding_model,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents\",\n",
    "    retrieval_mode=RetrievalMode.DENSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What datasets they used to benchmark LLAVA?\"\n",
    "found_docs = qdrant.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', '_id': '7a406f0fdf1642a9a57a3a33133636ba', '_collection_name': 'my_documents'}, page_content='C Training Details\\nWe pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a\\nbatch size of 128, and fine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a\\nlearning rate of 2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no\\nweight decay and a cosine learning rate with a warmup ratio of 3%. During finetuning, FSDP (Full\\nShard Data Parallel) and gradient checkpointing is used to save GPU memory, and offloading is not\\nused. BF16 and TF32 are enabled to achieve a balance between speed and precision.\\nWe train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning\\non Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\\nD Assets\\nOur source code, generated instruction-tuning data, proposed benchmark are uploaded to the\\nanonymized GitHub repository: LLaV A-Annonymous/LLaV A.\\n1. Source Code: link\\n2. README: link\\n3. Instructions to launch the demo: link\\n4. All prompts and few shot examples for querying GPT-4: link\\n5. LLaV A-Instruct-158K: link\\n6. LLaV A-Bench: COCO, In-The-Wild\\n7. Model checkpoints. The size of the model checkpoints after compression is 25GB, which\\nexceeds the 5GB limit of GitHub LFS (Large File Storage). We’ll release the checkpoint to\\nthe public, or upon request with reviewers for this submission.\\nE Data\\nInstructions for brief image description.The list of instructions used to briefly describe the image\\ncontent are shown in Table 11. They present the same meaning with natural language variance.\\n• \"Describe the image concisely.\"\\n• \"Provide a brief description of the given image.\"\\n• \"Offer a succinct explanation of the picture presented.\"\\n• \"Summarize the visual content of the image.\"\\n• \"Give a short and clear explanation of the subsequent image.\"\\n• \"Share a concise interpretation of the image provided.\"\\n• \"Present a compact description of the photo’s key features.\"\\n• \"Relay a brief, clear account of the picture shown.\"\\n• \"Render a clear and concise summary of the photo.\"\\n• \"Write a terse but informative summary of the picture.\"\\n• \"Create a compact narrative representing the image presented.\"\\nTable 11: The list of instructions for brief image description.\\nInstructions for detailed image description.The list of instructions used to describe the image\\ncontent in detail are shown in Table 12. They present the same meaning with natural language\\nvariance.\\nCC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and\\ncount the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller\\nthan 3, as they are usually rare combinations concept and attributes that has already been covered\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 13, 'page_label': '14', '_id': 'ea3254b0c6a54bb4b3aad4a906551a9b', '_collection_name': 'my_documents'}, page_content='A Broader Impact\\nThe broader impact of LLaV A, a general-purpose visual assistant, has potential benefits and risks\\nassociated with its deployment and release. Some considerations are unique to LLaV A due to its\\nvisual nature, while others share similarities with existing instruction-following LLMs (e.g., Alpaca,\\nVicuna, etc.). As LLaV A is built upon LLaMA, Vicuna, and CLIP, it inherits some of the issues\\nassociated with LLMs and vision encoders. In the following, we outline both the risks and mitigation\\nstrategies in place for the release of this model.\\nMalicious input. To minimize potential misuse and harmful consequences, we employ two pre-\\ncautionary measures for LLaV A: (1)OpenAI Filter APIfor user input text to prevent harmful or\\ninappropriate text instructions from being processed by the model, and (2) NSFW Filterfor uploaded\\nuser images to detect and block Not Safe For Work (NSFW) content or any other potentially harmful\\nimage inputs.\\nHallucination. Similar to LLMs, LLaV A might generate outputs that aren’t grounded in facts\\nor input data. This raises concerns about inferences made, especially in critical applications ( e.g.,\\nmedical).\\nBiases. Bias can be transferred from the base models to LLaV A, both from the vision encoder\\n(CLIP) and the language decoder (LLaMA/Vicuna). This may lead to biased outcomes or unfair\\nrepresentations of diverse content.\\nEnergy consumption. Though energy consumption is not a primary concern for LLaV A due to\\na smaller pretraining dataset (see details in Sec. C), it may become a concern when scaling up the\\npretraining dataset or increasing the model size, e.g., to a larger LLaMA version like the 65B model.\\nEvaluation complexities. Assessing the performance of LLaV A is challenging as it involves both\\nlanguage and visual tasks. Our evaluation benchmark covers several aspects, including accuracy,\\nconcept coverage, reasoning ability, and creativity. However, additional aspects need consideration,\\nsuch as the degree of visual content hallucination and fine-grained understanding of visual content.\\nWhile text-only GPT-4 based multimodal evaluation is consistent and accurate in our study, its\\nrobustness in different situations and capability to evaluate other unexplored aspects are subjects for\\nfuture work.\\nDespite these risks, we believe that the benefits of releasing LLaV A to the research community\\noutweigh the potential harm. It allows for ongoing investigation and improvement of the model and\\nengages the community in developing better mitigation strategies to address these concerns. Moreover,\\nthe release of LLaV A can spur the development of new applications and research directions, ultimately\\ncontributing to the progress and responsible deployment of foundation models in vision-language\\ntasks.\\nB More Results\\nWe present more qualitative results of LLaV A to analyze its emergent behaviors and observed\\nweaknesses. For more quantitative results of LLaV A on academic benchmarks, please refer to the\\nimproved baselines with visual instruction tuning [32]. In Table 9, LLaV A demonstrates a similar\\nbehavior as GPT-4 in another example from its paper. Similar to the GPT-4 live demo by OpenAI,\\nLLaV A is capable of generating the HTML/JS/CSS code for an interactive joke website based on\\na simplified user input sketch in Fig. 2, despite a minor error. As shown in Fig. 3, LLaV A can\\nfollow user’s instructions in a conversational style and provide detailed responses or creative writings.\\nFurthermore, LLaV A is able to relate the visual content to the textual knowledge from the pretrained\\nLLM, as demonstrated in Fig. 4 and Fig. 5.\\nOne interesting emergent behavior of LLaV A is that it is able to understand visual contents that\\nare not covered in the training. For example, in Fig. 6, it is able to recognize Elon Musk both in a\\nheadshot and in a humorous meme where he is dressed as a doge, even though Elon Musk never'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', '_id': 'b73659fdbfe34a50ac23c0f80fa50cdc', '_collection_name': 'my_documents'}, page_content='fridge example (right), perceiving the correct brand of the yogurt requires the model to process high\\nresolution images and possess extensive knowledge coverage. We also observed an interesting failure\\nof LLaV A, as it responds withyes when asked if strawberry-flavored yogurt is present, even though\\nthe fridge contains only yogurt and strawberries. This indicates that, at times, LLaV A perceives\\nthe image as a “bag of patches”, failing to grasp the complex semantics within the image. We hope\\nLLaV A serves as a solid baseline on the benchmarks, on which our findings can inspire future work\\nin developing more capable LMMs.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', '_id': '458a4264619640ecb8964368748d499c', '_collection_name': 'my_documents'}, page_content='LLaV A. LLaV A also demonstrates impressive OCR (optical character recognition) ability in Table 9\\nand Fig. 2, which is rarely covered in our training data.\\nWe hope these additional results and observations showcase the potential of LLaV A in various\\napplication areas. In future work, it is important to investigate these emergent behaviors more\\nthoroughly and to understand the underlying mechanisms that enable LLaV A to demonstrate such\\ngeneralization abilities. This will pave the way towards building better LMMs, including enhancing\\nrobustness, reducing biases, and improving the alignment and the scope of the learned vision-language\\nrepresentations.\\nVisual input example, Chicken Nugget Map:\\nUser Can you explain this meme in detail?\\nLLaV A The meme in the image is a creative and humorous take on food, with a focus on chicken\\nnuggets, as the center of the universe. The meme begins with the phrase \"Sometimes I just\\nlook at pictures of the Earth from space and I marvel at how beautiful it all is...\" The main\\nimage features a tray of cooked chicken nuggets, with the chicken pieces positioned to\\nresemble the continents and islands on a world map. The meme playfully suggests that the\\nchicken nuggets represent the Earth, and the various locations depicted in the photo are\\nactually chicken nugget versions of different places. The punchline of the meme appears\\nin smaller text at the bottom, reading \"I mean, it’s not the real Earth, but how beautiful it\\nis all is.\" This lighthearted and entertaining meme combines food and humor, using the\\nimage of chicken nuggets to create a funny and imaginative representation of the world.\\nUser Can you explain this meme?\\nGPT-4 [36] This meme is a joke that combines two unrelated things: pictures of the earth from space\\nand chicken nuggets. The text of the meme suggests that the image below is a beautiful\\npicture of the earth from space. However, the image is actually of chicken nuggets\\narranged to vaguely resemble a map of the world. The humor in this meme comes from\\nthe unexpected juxtaposition of the text and the image. The text sets up an expectation of a\\nmajestic image of the earth, but the image is actually something mundane and silly.\\nUser Can you explain this meme in detail?\\nBLIP-2 sometimes i just look at pictures of the earth from space and marvel how beautiful it is\\nUser Can you explain this meme in detail?\\nOpenFlamingo It’s a picture of a chicken nugget on the International Space Station.\\nTable 9: Example prompt comparing LLaV A, GPT-4, BLIP-2, and OpenFlamingo’s visual reasoning\\ncapabilities in understanding the humor. BLIP-2 and OpenFlamingo fail to follow the user’s\\ninstructions. LLaV A and GPT-4 both explain the meme and its humor, while GPT-4 produces a more\\nconcise answer.\\n15')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', '_id': '7a406f0fdf1642a9a57a3a33133636ba', '_collection_name': 'my_documents'}, page_content='C Training Details\\nWe pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a\\nbatch size of 128, and fine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a\\nlearning rate of 2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no\\nweight decay and a cosine learning rate with a warmup ratio of 3%. During finetuning, FSDP (Full\\nShard Data Parallel) and gradient checkpointing is used to save GPU memory, and offloading is not\\nused. BF16 and TF32 are enabled to achieve a balance between speed and precision.\\nWe train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning\\non Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\\nD Assets\\nOur source code, generated instruction-tuning data, proposed benchmark are uploaded to the\\nanonymized GitHub repository: LLaV A-Annonymous/LLaV A.\\n1. Source Code: link\\n2. README: link\\n3. Instructions to launch the demo: link\\n4. All prompts and few shot examples for querying GPT-4: link\\n5. LLaV A-Instruct-158K: link\\n6. LLaV A-Bench: COCO, In-The-Wild\\n7. Model checkpoints. The size of the model checkpoints after compression is 25GB, which\\nexceeds the 5GB limit of GitHub LFS (Large File Storage). We’ll release the checkpoint to\\nthe public, or upon request with reviewers for this submission.\\nE Data\\nInstructions for brief image description.The list of instructions used to briefly describe the image\\ncontent are shown in Table 11. They present the same meaning with natural language variance.\\n• \"Describe the image concisely.\"\\n• \"Provide a brief description of the given image.\"\\n• \"Offer a succinct explanation of the picture presented.\"\\n• \"Summarize the visual content of the image.\"\\n• \"Give a short and clear explanation of the subsequent image.\"\\n• \"Share a concise interpretation of the image provided.\"\\n• \"Present a compact description of the photo’s key features.\"\\n• \"Relay a brief, clear account of the picture shown.\"\\n• \"Render a clear and concise summary of the photo.\"\\n• \"Write a terse but informative summary of the picture.\"\\n• \"Create a compact narrative representing the image presented.\"\\nTable 11: The list of instructions for brief image description.\\nInstructions for detailed image description.The list of instructions used to describe the image\\ncontent in detail are shown in Table 12. They present the same meaning with natural language\\nvariance.\\nCC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and\\ncount the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller\\nthan 3, as they are usually rare combinations concept and attributes that has already been covered\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 13, 'page_label': '14', '_id': 'ea3254b0c6a54bb4b3aad4a906551a9b', '_collection_name': 'my_documents'}, page_content='A Broader Impact\\nThe broader impact of LLaV A, a general-purpose visual assistant, has potential benefits and risks\\nassociated with its deployment and release. Some considerations are unique to LLaV A due to its\\nvisual nature, while others share similarities with existing instruction-following LLMs (e.g., Alpaca,\\nVicuna, etc.). As LLaV A is built upon LLaMA, Vicuna, and CLIP, it inherits some of the issues\\nassociated with LLMs and vision encoders. In the following, we outline both the risks and mitigation\\nstrategies in place for the release of this model.\\nMalicious input. To minimize potential misuse and harmful consequences, we employ two pre-\\ncautionary measures for LLaV A: (1)OpenAI Filter APIfor user input text to prevent harmful or\\ninappropriate text instructions from being processed by the model, and (2) NSFW Filterfor uploaded\\nuser images to detect and block Not Safe For Work (NSFW) content or any other potentially harmful\\nimage inputs.\\nHallucination. Similar to LLMs, LLaV A might generate outputs that aren’t grounded in facts\\nor input data. This raises concerns about inferences made, especially in critical applications ( e.g.,\\nmedical).\\nBiases. Bias can be transferred from the base models to LLaV A, both from the vision encoder\\n(CLIP) and the language decoder (LLaMA/Vicuna). This may lead to biased outcomes or unfair\\nrepresentations of diverse content.\\nEnergy consumption. Though energy consumption is not a primary concern for LLaV A due to\\na smaller pretraining dataset (see details in Sec. C), it may become a concern when scaling up the\\npretraining dataset or increasing the model size, e.g., to a larger LLaMA version like the 65B model.\\nEvaluation complexities. Assessing the performance of LLaV A is challenging as it involves both\\nlanguage and visual tasks. Our evaluation benchmark covers several aspects, including accuracy,\\nconcept coverage, reasoning ability, and creativity. However, additional aspects need consideration,\\nsuch as the degree of visual content hallucination and fine-grained understanding of visual content.\\nWhile text-only GPT-4 based multimodal evaluation is consistent and accurate in our study, its\\nrobustness in different situations and capability to evaluate other unexplored aspects are subjects for\\nfuture work.\\nDespite these risks, we believe that the benefits of releasing LLaV A to the research community\\noutweigh the potential harm. It allows for ongoing investigation and improvement of the model and\\nengages the community in developing better mitigation strategies to address these concerns. Moreover,\\nthe release of LLaV A can spur the development of new applications and research directions, ultimately\\ncontributing to the progress and responsible deployment of foundation models in vision-language\\ntasks.\\nB More Results\\nWe present more qualitative results of LLaV A to analyze its emergent behaviors and observed\\nweaknesses. For more quantitative results of LLaV A on academic benchmarks, please refer to the\\nimproved baselines with visual instruction tuning [32]. In Table 9, LLaV A demonstrates a similar\\nbehavior as GPT-4 in another example from its paper. Similar to the GPT-4 live demo by OpenAI,\\nLLaV A is capable of generating the HTML/JS/CSS code for an interactive joke website based on\\na simplified user input sketch in Fig. 2, despite a minor error. As shown in Fig. 3, LLaV A can\\nfollow user’s instructions in a conversational style and provide detailed responses or creative writings.\\nFurthermore, LLaV A is able to relate the visual content to the textual knowledge from the pretrained\\nLLM, as demonstrated in Fig. 4 and Fig. 5.\\nOne interesting emergent behavior of LLaV A is that it is able to understand visual contents that\\nare not covered in the training. For example, in Fig. 6, it is able to recognize Elon Musk both in a\\nheadshot and in a humorous meme where he is dressed as a doge, even though Elon Musk never'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', '_id': 'b73659fdbfe34a50ac23c0f80fa50cdc', '_collection_name': 'my_documents'}, page_content='fridge example (right), perceiving the correct brand of the yogurt requires the model to process high\\nresolution images and possess extensive knowledge coverage. We also observed an interesting failure\\nof LLaV A, as it responds withyes when asked if strawberry-flavored yogurt is present, even though\\nthe fridge contains only yogurt and strawberries. This indicates that, at times, LLaV A perceives\\nthe image as a “bag of patches”, failing to grasp the complex semantics within the image. We hope\\nLLaV A serves as a solid baseline on the benchmarks, on which our findings can inspire future work\\nin developing more capable LMMs.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', '_id': '458a4264619640ecb8964368748d499c', '_collection_name': 'my_documents'}, page_content='LLaV A. LLaV A also demonstrates impressive OCR (optical character recognition) ability in Table 9\\nand Fig. 2, which is rarely covered in our training data.\\nWe hope these additional results and observations showcase the potential of LLaV A in various\\napplication areas. In future work, it is important to investigate these emergent behaviors more\\nthoroughly and to understand the underlying mechanisms that enable LLaV A to demonstrate such\\ngeneralization abilities. This will pave the way towards building better LMMs, including enhancing\\nrobustness, reducing biases, and improving the alignment and the scope of the learned vision-language\\nrepresentations.\\nVisual input example, Chicken Nugget Map:\\nUser Can you explain this meme in detail?\\nLLaV A The meme in the image is a creative and humorous take on food, with a focus on chicken\\nnuggets, as the center of the universe. The meme begins with the phrase \"Sometimes I just\\nlook at pictures of the Earth from space and I marvel at how beautiful it all is...\" The main\\nimage features a tray of cooked chicken nuggets, with the chicken pieces positioned to\\nresemble the continents and islands on a world map. The meme playfully suggests that the\\nchicken nuggets represent the Earth, and the various locations depicted in the photo are\\nactually chicken nugget versions of different places. The punchline of the meme appears\\nin smaller text at the bottom, reading \"I mean, it’s not the real Earth, but how beautiful it\\nis all is.\" This lighthearted and entertaining meme combines food and humor, using the\\nimage of chicken nuggets to create a funny and imaginative representation of the world.\\nUser Can you explain this meme?\\nGPT-4 [36] This meme is a joke that combines two unrelated things: pictures of the earth from space\\nand chicken nuggets. The text of the meme suggests that the image below is a beautiful\\npicture of the earth from space. However, the image is actually of chicken nuggets\\narranged to vaguely resemble a map of the world. The humor in this meme comes from\\nthe unexpected juxtaposition of the text and the image. The text sets up an expectation of a\\nmajestic image of the earth, but the image is actually something mundane and silly.\\nUser Can you explain this meme in detail?\\nBLIP-2 sometimes i just look at pictures of the earth from space and marvel how beautiful it is\\nUser Can you explain this meme in detail?\\nOpenFlamingo It’s a picture of a chicken nugget on the International Space Station.\\nTable 9: Example prompt comparing LLaV A, GPT-4, BLIP-2, and OpenFlamingo’s visual reasoning\\ncapabilities in understanding the humor. BLIP-2 and OpenFlamingo fail to follow the user’s\\ninstructions. LLaV A and GPT-4 both explain the meme and its humor, while GPT-4 produces a more\\nconcise answer.\\n15')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = qdrant.as_retriever()\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Vector Search\n",
    "\n",
    "To search with only sparse vectors,\n",
    "\n",
    "The retrieval_mode parameter should be set to RetrievalMode.SPARSE.\n",
    "An implementation of the SparseEmbeddings interface using any sparse embeddings provider has to be provided as value to the sparse_embedding parameter.\n",
    "The langchain-qdrant package provides a FastEmbed based implementation out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\", cache_dir=\"cache\")\n",
    "\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    docs,\n",
    "    embedding=embedding_model,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents\",\n",
    "    retrieval_mode=RetrievalMode.SPARSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_docs = qdrant.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', '_id': '622942565ee04743966c76b7b42fa1aa', '_collection_name': 'my_documents'}, page_content='C Training Details\\nWe pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a\\nbatch size of 128, and fine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a\\nlearning rate of 2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no\\nweight decay and a cosine learning rate with a warmup ratio of 3%. During finetuning, FSDP (Full\\nShard Data Parallel) and gradient checkpointing is used to save GPU memory, and offloading is not\\nused. BF16 and TF32 are enabled to achieve a balance between speed and precision.\\nWe train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning\\non Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\\nD Assets\\nOur source code, generated instruction-tuning data, proposed benchmark are uploaded to the\\nanonymized GitHub repository: LLaV A-Annonymous/LLaV A.\\n1. Source Code: link\\n2. README: link\\n3. Instructions to launch the demo: link\\n4. All prompts and few shot examples for querying GPT-4: link\\n5. LLaV A-Instruct-158K: link\\n6. LLaV A-Bench: COCO, In-The-Wild\\n7. Model checkpoints. The size of the model checkpoints after compression is 25GB, which\\nexceeds the 5GB limit of GitHub LFS (Large File Storage). We’ll release the checkpoint to\\nthe public, or upon request with reviewers for this submission.\\nE Data\\nInstructions for brief image description.The list of instructions used to briefly describe the image\\ncontent are shown in Table 11. They present the same meaning with natural language variance.\\n• \"Describe the image concisely.\"\\n• \"Provide a brief description of the given image.\"\\n• \"Offer a succinct explanation of the picture presented.\"\\n• \"Summarize the visual content of the image.\"\\n• \"Give a short and clear explanation of the subsequent image.\"\\n• \"Share a concise interpretation of the image provided.\"\\n• \"Present a compact description of the photo’s key features.\"\\n• \"Relay a brief, clear account of the picture shown.\"\\n• \"Render a clear and concise summary of the photo.\"\\n• \"Write a terse but informative summary of the picture.\"\\n• \"Create a compact narrative representing the image presented.\"\\nTable 11: The list of instructions for brief image description.\\nInstructions for detailed image description.The list of instructions used to describe the image\\ncontent in detail are shown in Table 12. They present the same meaning with natural language\\nvariance.\\nCC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and\\ncount the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller\\nthan 3, as they are usually rare combinations concept and attributes that has already been covered\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', '_id': '3bdccdc30a5446309609d179e28eeda8', '_collection_name': 'my_documents'}, page_content='Xsystem-message <STOP>\\nHuman : X1\\ninstruct <STOP> Assistant: X1\\na <STOP>\\nHuman : X2\\ninstruct <STOP> Assistant: X2\\na <STOP> · · ·\\nTable 2: The input sequence used to train the model. Only two conversation turns are illustrated\\nhere; in practice, the number of turns varies based on the instruction-following data. In our current\\nimplementation, we follow Vicuna-v0 [ 9] to set the system message Xsystem-message and we set\\n<STOP> = ###. The model is trained to predict the assistant answers and where to stop, and thus\\nonly green sequence/tokens are used to compute the loss in the auto-regressive model.\\nwhere θ is the trainable parameters, Xinstruct,<i and Xa,<i are the instruction and answer tokens in\\nall turns before the current prediction token xi, respectively. Please see Table 2 for an illustration of\\nthe prediction tokens. For the conditionals in (3), we explicitly add Xv to emphasize the fact that the\\nimage is grounded for all answers, and we omit Xsystem-message and all previous <STOP> for better\\nreadability. For LLaV A model training, we consider a two-stage instruction-tuning procedure.\\nStage 1: Pre-training for Feature Alignment.To strike a balance between concept coverage\\nand training efficiency, we filter CC3M to 595K image-text pairs. Please see Appendix for details\\nof the filtering process. These pairs are converted to the instruction-following data using the naive\\nexpansion method describe in Section 3. Each sample can be treated as a single-turn conversation. To\\nconstruct the input Xinstruct in (2), for an image Xv, a question Xq is randomly sampled, which is a\\nlanguage instruction to request the assistant to describe the image briefly. The ground-truth prediction\\nanswer Xa is the original caption. In training, we keep both the visual encoder and LLM weights\\nfrozen, and maximize the likelihood of (3) with trainable parameters θ = W (the projection matrix)\\nonly. In this way, the image features Hv can be aligned with the pre-trained LLM word embedding.\\nThis stage can be understood as training a compatible visual tokenizer for the frozen LLM.\\nStage 2: Fine-tuning End-to-End.We always keep the visual encoder weights frozen, and continue\\nto update both the pre-trained weights of the projection layer and LLM in LLaV A; i.e., the trainable\\nparameters are θ = {W, ϕ} in (3). We consider two specific use case scenarios:\\n• Multimodal Chatbot. We develop a Chatbot by fine-tuning on the 158K language-image\\ninstruction-following data in Section 3. Among the three types of responses, conversation is\\nmulti-turn while the other two are single-turn. They are uniformly sampled in training.\\n• Science QA. We study our method on the ScienceQA benchmark [ 34], the first large-scale\\nmultimodal science question dataset that annotates the answers with detailed lectures and\\nexplanations. Each question is provided a context in the form of natural language or an image.\\nThe assistant provides the reasoning process in natural language and selects the answer among\\nmultiple choices. For training in (2), we organize the data as a single turn conversation, the\\nquestion & context as Xinstruct, and reasoning & answer as Xa.\\n5 Experiments\\nWe assess the performance of LLaV A in instruction-following and visual reasoning capabilities with\\ntwo primary experimental settings: multimodal chatbot and the ScienceQA dataset, respectively. We\\ntrain all models with 8× A100s, following Vicuna’s hyperparameters [9]. We pre-train our model\\non the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a batch size of 128, and\\nfine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a learning rate of 2e-5 and\\na batch size of 32. See Appendix for more training details.\\n5.1 Multimodal Chatbot\\nWe developed a chatbot demo to show the image understanding and conversation abilities of LLaV A,\\nand to study how well LLaV A is able to digest visual inputs and exhibit instruction-following'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', '_id': '5569d3e6959544219904d84ef888aae5', '_collection_name': 'my_documents'}, page_content='Visual Instruction Tuning\\nHaotian Liu1∗, Chunyuan Li2∗, Qingyang Wu3, Yong Jae Lee1\\n1University of Wisconsin–Madison 2Microsoft Research 3Columbia University\\nhttps://llava-vl.github.io\\nAbstract\\nInstruction tuning large language models (LLMs) using machine-generated\\ninstruction-following data has been shown to improve zero-shot capabilities on\\nnew tasks, but the idea is less explored in the multimodal field. We present the\\nfirst attempt to use language-only GPT-4 to generate multimodal language-image\\ninstruction-following data. By instruction tuning on such generated data, we in-\\ntroduce LLaV A:Large Language and Vision Assistant, an end-to-end trained\\nlarge multimodal model that connects a vision encoder and an LLM for general-\\npurpose visual and language understanding. To facilitate future research on visual\\ninstruction following, we construct two evaluation benchmarks with diverse and\\nchallenging application-oriented tasks. Our experiments show that LLaV A demon-\\nstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors\\nof multimodal GPT-4 on unseen images/instructions, and yields a 85.1% rela-\\ntive score compared with GPT-4 on a synthetic multimodal instruction-following\\ndataset. When fine-tuned on Science QA, the synergy of LLaV A and GPT-4\\nachieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated\\nvisual instruction tuning data, our model, and code publicly available.\\n1 Introduction\\nHumans interact with the world through many channels such as vision and language, as each\\nindividual channel has a unique advantage in representing and communicating certain concepts, and\\nthus facilitates a better understanding of the world. One of the core aspirations in artificial intelligence\\nis to develop a general-purpose assistant that can effectively follow multi-modal vision-and-language\\ninstructions, aligned with human intent to complete various real-world tasks in the wild [4, 27, 26].\\nTo this end, the community has witnessed an emergent interest in developing language-augmented\\nfoundation vision models [ 27, 16], with strong capabilities in open-world visual understanding\\nsuch as classification [ 40, 21, 57, 54, 39], detection [ 29, 62, 33], segmentation [ 25, 63, 58] and\\ncaptioning [50, 28], as well as visual generation and editing [42, 43, 56, 15, 44, 30]. We refer readers\\nto the Computer Vision in the Wildreading list for a more up-to-date literature compilation [12]. In\\nthis line of work, each task is solved independently by one single large vision model, with the task\\ninstruction implicitly considered in the model design. Further, language is only utilized to describe\\nthe image content. While this allows language to play an important role in mapping visual signals to\\nlanguage semantics—a common channel for human communication, it leads to models that usually\\nhave a fixed interface with limited interactivity and adaptability to the user’s instructions.\\nLarge language models (LLM), on the other hand, have shown that language can play a wider\\nrole: a universal interface for a general-purpose assistant, where various task instructions can be\\nexplicitly represented in language and guide the end-to-end trained neural assistant to switch to the\\ntask of interest to solve it. For example, the recent success of ChatGPT [35] and GPT-4 [36] have\\ndemonstrated the power of aligned LLMs in following human instructions, and have stimulated\\ntremendous interest in developing open-source LLMs. Among them, LLaMA [ 49] is an open-\\nsource LLM that matches the performance of GPT-3. Alpaca [ 48], Vicuna [9], GPT-4-LLM [38]\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\\narXiv:2304.08485v2  [cs.CV]  11 Dec 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', '_id': 'df1914636a9a49e38a9fb8f72186d661', '_collection_name': 'my_documents'}, page_content='Challenging examples from LLaV A-Bench (In-the-Wild):\\nICHIRAN Ramen [source] Filled fridge [source]\\nAnnotation A close-up photo of a meal at ICHI-\\nRAN. The chashu ramen bowl with\\na spoon is placed in the center. The\\nramen is seasoned with chili sauce,\\nchopped scallions, and served with\\ntwo pieces of chashu. Chopsticks are\\nplaced to the right of the bowl, still in\\ntheir paper wrap, not yet opened. The\\nramen is also served with nori on the\\nleft. On top, from left to right, the fol-\\nlowing sides are served: a bowl of or-\\nange spice (possibly garlic sauce), a\\nplate of smoke-flavored stewed pork\\nwith chopped scallions, and a cup of\\nmatcha green tea.\\nAn open refrigerator filled with a variety of food\\nitems. In the left part of the compartment, towards\\nthe front, there is a plastic box of strawberries with a\\nsmall bag of baby carrots on top. Towards the back,\\nthere is a stack of sauce containers. In the middle\\npart of the compartment, towards the front, there\\nis a green plastic box, and there is an unidentified\\nplastic bag placed on it. Towards the back, there is a\\ncarton of milk. In the right part of the compartment,\\ntowards the front, there is a box of blueberries with\\nthree yogurts stacked on top. The large bottle of\\nyogurt is Fage non-fat yogurt, and one of the smaller\\ncups is Fage blueberry yogurt. The brand and flavor\\nof the other smaller cup are unknown. Towards the\\nback, there is a container with an unknown content.\\nQuestion 1 What’s the name of the restaurant? What is the brand of the blueberry-flavored yogurt?\\nQuestion 2 Describe this photo in detail. Is there strawberry-flavored yogurt in the fridge?\\nTable 6: Challenging examples from LLaV A-Bench (In-the-Wild), we provide extremely-detailed\\nannotation for each image for an accurate evaluation. Some questions require the model to extract\\ndetails from high resolution image and to have a broad knowledge coverage.\\n5.2 ScienceQA\\nScienceQA [34] contains 21k multimodal multiple choice questions with rich domain diversity across\\n3 subjects, 26 topics, 127 categories, and 379 skills. The benchmark dataset is split into training,\\nvalidation, and test splits with 12726, 4241, and 4241 examples, respectively. We consider two\\nrepresentative methods, including GPT-3.5 model (text-davinci-002) with and without chain-\\nof-thought (CoT), LLaMA-Adapter [59], as well as multimodal chain-of-thought (MM-CoT) [61],\\nwhich is the current SoTA method on this dataset. For more baseline numbers, please see [34].\\nThe results are reported in Table 7. For LLaV A, we use the visual features before the last layer, ask\\nthe model to first predict reasons and then the answer, and train it for 12 epochs. It yields 90.92%\\naccuracy, which is quite close to the SoTA 91.68%. To explore the limit of LLMs, we also prompt\\nGPT-4 using 2-shot in-context-learning and achieve 82.69% accuracy, which is a 7.52% absolute gain\\ncompared with 75.17% from GPT-3.5. For a substantial number of questions, we note that GPT-4 fails\\nsimply because it reports that there is insufficient context such as images or plots. We consider two\\nschemes to combine the outcomes from our model and GPT-4. (i) A GPT-4 complement. Whenever\\nGPT-4 fails to provide answers, we use the prediction from our method. This schemes yields 90.97%\\naccuracy, which is almost the same as applying our method alone.(ii) GPT-4 as the judge. Whenever\\nGPT-4 and LLaV A produce different answers, we prompt GPT-4 again, asking it to provide its own\\nfinal answer based on the question and two outcomes. The spirit is similar with CoT, but with the\\nexternal knowledge from the other model. Surprisingly, this scheme is able to provide consistent\\nimprovement over all question classes, and achieves a new SoTA accuracy of 92.53%. Interestingly,\\nthe text-only GPT-4, which cannot process images, improves the overall performance of the model\\non questions that have an image as context. This is because some of these questions do not actually')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Search\n",
    "\n",
    "To perform a hybrid search using dense and sparse vectors with score fusion,\n",
    "\n",
    "The retrieval_mode parameter should be set to RetrievalMode.HYBRID.\n",
    "A dense embeddings value should be provided to the embedding parameter.\n",
    "An implementation of the SparseEmbeddings interface using any sparse embeddings provider has to be provided as value to the sparse_embedding parameter.\n",
    "Note that if you've added documents with the HYBRID mode, you can switch to any retrieval mode when searching. Since both the dense and sparse vectors are available in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 18 files: 100%|██████████| 18/18 [00:01<00:00, 13.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    docs,\n",
    "    embedding=embedding_model,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents\",\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_docs = qdrant.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', '_id': 'cee7ed6308184fe5a0770176ca5d54c7', '_collection_name': 'my_documents'}, page_content='C Training Details\\nWe pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a\\nbatch size of 128, and fine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a\\nlearning rate of 2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no\\nweight decay and a cosine learning rate with a warmup ratio of 3%. During finetuning, FSDP (Full\\nShard Data Parallel) and gradient checkpointing is used to save GPU memory, and offloading is not\\nused. BF16 and TF32 are enabled to achieve a balance between speed and precision.\\nWe train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning\\non Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\\nD Assets\\nOur source code, generated instruction-tuning data, proposed benchmark are uploaded to the\\nanonymized GitHub repository: LLaV A-Annonymous/LLaV A.\\n1. Source Code: link\\n2. README: link\\n3. Instructions to launch the demo: link\\n4. All prompts and few shot examples for querying GPT-4: link\\n5. LLaV A-Instruct-158K: link\\n6. LLaV A-Bench: COCO, In-The-Wild\\n7. Model checkpoints. The size of the model checkpoints after compression is 25GB, which\\nexceeds the 5GB limit of GitHub LFS (Large File Storage). We’ll release the checkpoint to\\nthe public, or upon request with reviewers for this submission.\\nE Data\\nInstructions for brief image description.The list of instructions used to briefly describe the image\\ncontent are shown in Table 11. They present the same meaning with natural language variance.\\n• \"Describe the image concisely.\"\\n• \"Provide a brief description of the given image.\"\\n• \"Offer a succinct explanation of the picture presented.\"\\n• \"Summarize the visual content of the image.\"\\n• \"Give a short and clear explanation of the subsequent image.\"\\n• \"Share a concise interpretation of the image provided.\"\\n• \"Present a compact description of the photo’s key features.\"\\n• \"Relay a brief, clear account of the picture shown.\"\\n• \"Render a clear and concise summary of the photo.\"\\n• \"Write a terse but informative summary of the picture.\"\\n• \"Create a compact narrative representing the image presented.\"\\nTable 11: The list of instructions for brief image description.\\nInstructions for detailed image description.The list of instructions used to describe the image\\ncontent in detail are shown in Table 12. They present the same meaning with natural language\\nvariance.\\nCC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and\\ncount the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller\\nthan 3, as they are usually rare combinations concept and attributes that has already been covered\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 13, 'page_label': '14', '_id': '51cac3eef16948f6b9f082ce115bc510', '_collection_name': 'my_documents'}, page_content='A Broader Impact\\nThe broader impact of LLaV A, a general-purpose visual assistant, has potential benefits and risks\\nassociated with its deployment and release. Some considerations are unique to LLaV A due to its\\nvisual nature, while others share similarities with existing instruction-following LLMs (e.g., Alpaca,\\nVicuna, etc.). As LLaV A is built upon LLaMA, Vicuna, and CLIP, it inherits some of the issues\\nassociated with LLMs and vision encoders. In the following, we outline both the risks and mitigation\\nstrategies in place for the release of this model.\\nMalicious input. To minimize potential misuse and harmful consequences, we employ two pre-\\ncautionary measures for LLaV A: (1)OpenAI Filter APIfor user input text to prevent harmful or\\ninappropriate text instructions from being processed by the model, and (2) NSFW Filterfor uploaded\\nuser images to detect and block Not Safe For Work (NSFW) content or any other potentially harmful\\nimage inputs.\\nHallucination. Similar to LLMs, LLaV A might generate outputs that aren’t grounded in facts\\nor input data. This raises concerns about inferences made, especially in critical applications ( e.g.,\\nmedical).\\nBiases. Bias can be transferred from the base models to LLaV A, both from the vision encoder\\n(CLIP) and the language decoder (LLaMA/Vicuna). This may lead to biased outcomes or unfair\\nrepresentations of diverse content.\\nEnergy consumption. Though energy consumption is not a primary concern for LLaV A due to\\na smaller pretraining dataset (see details in Sec. C), it may become a concern when scaling up the\\npretraining dataset or increasing the model size, e.g., to a larger LLaMA version like the 65B model.\\nEvaluation complexities. Assessing the performance of LLaV A is challenging as it involves both\\nlanguage and visual tasks. Our evaluation benchmark covers several aspects, including accuracy,\\nconcept coverage, reasoning ability, and creativity. However, additional aspects need consideration,\\nsuch as the degree of visual content hallucination and fine-grained understanding of visual content.\\nWhile text-only GPT-4 based multimodal evaluation is consistent and accurate in our study, its\\nrobustness in different situations and capability to evaluate other unexplored aspects are subjects for\\nfuture work.\\nDespite these risks, we believe that the benefits of releasing LLaV A to the research community\\noutweigh the potential harm. It allows for ongoing investigation and improvement of the model and\\nengages the community in developing better mitigation strategies to address these concerns. Moreover,\\nthe release of LLaV A can spur the development of new applications and research directions, ultimately\\ncontributing to the progress and responsible deployment of foundation models in vision-language\\ntasks.\\nB More Results\\nWe present more qualitative results of LLaV A to analyze its emergent behaviors and observed\\nweaknesses. For more quantitative results of LLaV A on academic benchmarks, please refer to the\\nimproved baselines with visual instruction tuning [32]. In Table 9, LLaV A demonstrates a similar\\nbehavior as GPT-4 in another example from its paper. Similar to the GPT-4 live demo by OpenAI,\\nLLaV A is capable of generating the HTML/JS/CSS code for an interactive joke website based on\\na simplified user input sketch in Fig. 2, despite a minor error. As shown in Fig. 3, LLaV A can\\nfollow user’s instructions in a conversational style and provide detailed responses or creative writings.\\nFurthermore, LLaV A is able to relate the visual content to the textual knowledge from the pretrained\\nLLM, as demonstrated in Fig. 4 and Fig. 5.\\nOne interesting emergent behavior of LLaV A is that it is able to understand visual contents that\\nare not covered in the training. For example, in Fig. 6, it is able to recognize Elon Musk both in a\\nheadshot and in a humorous meme where he is dressed as a doge, even though Elon Musk never'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', '_id': 'f3c146b812974f9aa94133b45a8a41e1', '_collection_name': 'my_documents'}, page_content='Xsystem-message <STOP>\\nHuman : X1\\ninstruct <STOP> Assistant: X1\\na <STOP>\\nHuman : X2\\ninstruct <STOP> Assistant: X2\\na <STOP> · · ·\\nTable 2: The input sequence used to train the model. Only two conversation turns are illustrated\\nhere; in practice, the number of turns varies based on the instruction-following data. In our current\\nimplementation, we follow Vicuna-v0 [ 9] to set the system message Xsystem-message and we set\\n<STOP> = ###. The model is trained to predict the assistant answers and where to stop, and thus\\nonly green sequence/tokens are used to compute the loss in the auto-regressive model.\\nwhere θ is the trainable parameters, Xinstruct,<i and Xa,<i are the instruction and answer tokens in\\nall turns before the current prediction token xi, respectively. Please see Table 2 for an illustration of\\nthe prediction tokens. For the conditionals in (3), we explicitly add Xv to emphasize the fact that the\\nimage is grounded for all answers, and we omit Xsystem-message and all previous <STOP> for better\\nreadability. For LLaV A model training, we consider a two-stage instruction-tuning procedure.\\nStage 1: Pre-training for Feature Alignment.To strike a balance between concept coverage\\nand training efficiency, we filter CC3M to 595K image-text pairs. Please see Appendix for details\\nof the filtering process. These pairs are converted to the instruction-following data using the naive\\nexpansion method describe in Section 3. Each sample can be treated as a single-turn conversation. To\\nconstruct the input Xinstruct in (2), for an image Xv, a question Xq is randomly sampled, which is a\\nlanguage instruction to request the assistant to describe the image briefly. The ground-truth prediction\\nanswer Xa is the original caption. In training, we keep both the visual encoder and LLM weights\\nfrozen, and maximize the likelihood of (3) with trainable parameters θ = W (the projection matrix)\\nonly. In this way, the image features Hv can be aligned with the pre-trained LLM word embedding.\\nThis stage can be understood as training a compatible visual tokenizer for the frozen LLM.\\nStage 2: Fine-tuning End-to-End.We always keep the visual encoder weights frozen, and continue\\nto update both the pre-trained weights of the projection layer and LLM in LLaV A; i.e., the trainable\\nparameters are θ = {W, ϕ} in (3). We consider two specific use case scenarios:\\n• Multimodal Chatbot. We develop a Chatbot by fine-tuning on the 158K language-image\\ninstruction-following data in Section 3. Among the three types of responses, conversation is\\nmulti-turn while the other two are single-turn. They are uniformly sampled in training.\\n• Science QA. We study our method on the ScienceQA benchmark [ 34], the first large-scale\\nmultimodal science question dataset that annotates the answers with detailed lectures and\\nexplanations. Each question is provided a context in the form of natural language or an image.\\nThe assistant provides the reasoning process in natural language and selects the answer among\\nmultiple choices. For training in (2), we organize the data as a single turn conversation, the\\nquestion & context as Xinstruct, and reasoning & answer as Xa.\\n5 Experiments\\nWe assess the performance of LLaV A in instruction-following and visual reasoning capabilities with\\ntwo primary experimental settings: multimodal chatbot and the ScienceQA dataset, respectively. We\\ntrain all models with 8× A100s, following Vicuna’s hyperparameters [9]. We pre-train our model\\non the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a batch size of 128, and\\nfine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a learning rate of 2e-5 and\\na batch size of 32. See Appendix for more training details.\\n5.1 Multimodal Chatbot\\nWe developed a chatbot demo to show the image understanding and conversation abilities of LLaV A,\\nand to study how well LLaV A is able to digest visual inputs and exhibit instruction-following'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', '_id': '599e67cf8c1e484ba21b345ba67c1e7e', '_collection_name': 'my_documents'}, page_content='fridge example (right), perceiving the correct brand of the yogurt requires the model to process high\\nresolution images and possess extensive knowledge coverage. We also observed an interesting failure\\nof LLaV A, as it responds withyes when asked if strawberry-flavored yogurt is present, even though\\nthe fridge contains only yogurt and strawberries. This indicates that, at times, LLaV A perceives\\nthe image as a “bag of patches”, failing to grasp the complex semantics within the image. We hope\\nLLaV A serves as a solid baseline on the benchmarks, on which our findings can inspire future work\\nin developing more capable LMMs.\\n7')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.445506] * C Training Details\n",
      "We pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a\n",
      "batch size of 128, and fine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a\n",
      "learning rate of 2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no\n",
      "weight decay and a cosine learning rate with a warmup ratio of 3%. During finetuning, FSDP (Full\n",
      "Shard Data Parallel) and gradient checkpointing is used to save GPU memory, and offloading is not\n",
      "used. BF16 and TF32 are enabled to achieve a balance between speed and precision.\n",
      "We train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning\n",
      "on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\n",
      "D Assets\n",
      "Our source code, generated instruction-tuning data, proposed benchmark are uploaded to the\n",
      "anonymized GitHub repository: LLaV A-Annonymous/LLaV A.\n",
      "1. Source Code: link\n",
      "2. README: link\n",
      "3. Instructions to launch the demo: link\n",
      "4. All prompts and few shot examples for querying GPT-4: link\n",
      "5. LLaV A-Instruct-158K: link\n",
      "6. LLaV A-Bench: COCO, In-The-Wild\n",
      "7. Model checkpoints. The size of the model checkpoints after compression is 25GB, which\n",
      "exceeds the 5GB limit of GitHub LFS (Large File Storage). We’ll release the checkpoint to\n",
      "the public, or upon request with reviewers for this submission.\n",
      "E Data\n",
      "Instructions for brief image description.The list of instructions used to briefly describe the image\n",
      "content are shown in Table 11. They present the same meaning with natural language variance.\n",
      "• \"Describe the image concisely.\"\n",
      "• \"Provide a brief description of the given image.\"\n",
      "• \"Offer a succinct explanation of the picture presented.\"\n",
      "• \"Summarize the visual content of the image.\"\n",
      "• \"Give a short and clear explanation of the subsequent image.\"\n",
      "• \"Share a concise interpretation of the image provided.\"\n",
      "• \"Present a compact description of the photo’s key features.\"\n",
      "• \"Relay a brief, clear account of the picture shown.\"\n",
      "• \"Render a clear and concise summary of the photo.\"\n",
      "• \"Write a terse but informative summary of the picture.\"\n",
      "• \"Create a compact narrative representing the image presented.\"\n",
      "Table 11: The list of instructions for brief image description.\n",
      "Instructions for detailed image description.The list of instructions used to describe the image\n",
      "content in detail are shown in Table 12. They present the same meaning with natural language\n",
      "variance.\n",
      "CC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and\n",
      "count the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller\n",
      "than 3, as they are usually rare combinations concept and attributes that has already been covered\n",
      "20 [{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', '_id': '9c1b515339ff4f05bf5eb8d0045d13f0', '_collection_name': 'demo_collection'}]\n"
     ]
    }
   ],
   "source": [
    "#If you want to execute a similarity search and receive the corresponding scores you can run:\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    query=query, k=1\n",
    ")\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] * {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* C Training Details\n",
      "We pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a\n",
      "batch size of 128, and fine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a\n",
      "learning rate of 2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no\n",
      "weight decay and a cosine learning rate with a warmup ratio of 3%. During finetuning, FSDP (Full\n",
      "Shard Data Parallel) and gradient checkpointing is used to save GPU memory, and offloading is not\n",
      "used. BF16 and TF32 are enabled to achieve a balance between speed and precision.\n",
      "We train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning\n",
      "on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\n",
      "D Assets\n",
      "Our source code, generated instruction-tuning data, proposed benchmark are uploaded to the\n",
      "anonymized GitHub repository: LLaV A-Annonymous/LLaV A.\n",
      "1. Source Code: link\n",
      "2. README: link\n",
      "3. Instructions to launch the demo: link\n",
      "4. All prompts and few shot examples for querying GPT-4: link\n",
      "5. LLaV A-Instruct-158K: link\n",
      "6. LLaV A-Bench: COCO, In-The-Wild\n",
      "7. Model checkpoints. The size of the model checkpoints after compression is 25GB, which\n",
      "exceeds the 5GB limit of GitHub LFS (Large File Storage). We’ll release the checkpoint to\n",
      "the public, or upon request with reviewers for this submission.\n",
      "E Data\n",
      "Instructions for brief image description.The list of instructions used to briefly describe the image\n",
      "content are shown in Table 11. They present the same meaning with natural language variance.\n",
      "• \"Describe the image concisely.\"\n",
      "• \"Provide a brief description of the given image.\"\n",
      "• \"Offer a succinct explanation of the picture presented.\"\n",
      "• \"Summarize the visual content of the image.\"\n",
      "• \"Give a short and clear explanation of the subsequent image.\"\n",
      "• \"Share a concise interpretation of the image provided.\"\n",
      "• \"Present a compact description of the photo’s key features.\"\n",
      "• \"Relay a brief, clear account of the picture shown.\"\n",
      "• \"Render a clear and concise summary of the photo.\"\n",
      "• \"Write a terse but informative summary of the picture.\"\n",
      "• \"Create a compact narrative representing the image presented.\"\n",
      "Table 11: The list of instructions for brief image description.\n",
      "Instructions for detailed image description.The list of instructions used to describe the image\n",
      "content in detail are shown in Table 12. They present the same meaning with natural language\n",
      "variance.\n",
      "CC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and\n",
      "count the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller\n",
      "than 3, as they are usually rare combinations concept and attributes that has already been covered\n",
      "20 [{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', '_id': '9c1b515339ff4f05bf5eb8d0045d13f0', '_collection_name': 'demo_collection'}]\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.http import models\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=1,\n",
    ")\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', '_id': '9c1b515339ff4f05bf5eb8d0045d13f0', '_collection_name': 'demo_collection'}, page_content='C Training Details\\nWe pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a\\nbatch size of 128, and fine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a\\nlearning rate of 2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no\\nweight decay and a cosine learning rate with a warmup ratio of 3%. During finetuning, FSDP (Full\\nShard Data Parallel) and gradient checkpointing is used to save GPU memory, and offloading is not\\nused. BF16 and TF32 are enabled to achieve a balance between speed and precision.\\nWe train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning\\non Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\\nD Assets\\nOur source code, generated instruction-tuning data, proposed benchmark are uploaded to the\\nanonymized GitHub repository: LLaV A-Annonymous/LLaV A.\\n1. Source Code: link\\n2. README: link\\n3. Instructions to launch the demo: link\\n4. All prompts and few shot examples for querying GPT-4: link\\n5. LLaV A-Instruct-158K: link\\n6. LLaV A-Bench: COCO, In-The-Wild\\n7. Model checkpoints. The size of the model checkpoints after compression is 25GB, which\\nexceeds the 5GB limit of GitHub LFS (Large File Storage). We’ll release the checkpoint to\\nthe public, or upon request with reviewers for this submission.\\nE Data\\nInstructions for brief image description.The list of instructions used to briefly describe the image\\ncontent are shown in Table 11. They present the same meaning with natural language variance.\\n• \"Describe the image concisely.\"\\n• \"Provide a brief description of the given image.\"\\n• \"Offer a succinct explanation of the picture presented.\"\\n• \"Summarize the visual content of the image.\"\\n• \"Give a short and clear explanation of the subsequent image.\"\\n• \"Share a concise interpretation of the image provided.\"\\n• \"Present a compact description of the photo’s key features.\"\\n• \"Relay a brief, clear account of the picture shown.\"\\n• \"Render a clear and concise summary of the photo.\"\\n• \"Write a terse but informative summary of the picture.\"\\n• \"Create a compact narrative representing the image presented.\"\\nTable 11: The list of instructions for brief image description.\\nInstructions for detailed image description.The list of instructions used to describe the image\\ncontent in detail are shown in Table 12. They present the same meaning with natural language\\nvariance.\\nCC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and\\ncount the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller\\nthan 3, as they are usually rare combinations concept and attributes that has already been covered\\n20')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* C Training Details\n",
      "We pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a\n",
      "batch size of 128, and fine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a\n",
      "learning rate of 2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no\n",
      "weight decay and a cosine learning rate with a warmup ratio of 3%. During finetuning, FSDP (Full\n",
      "Shard Data Parallel) and gradient checkpointing is used to save GPU memory, and offloading is not\n",
      "used. BF16 and TF32 are enabled to achieve a balance between speed and precision.\n",
      "We train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning\n",
      "on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\n",
      "D Assets\n",
      "Our source code, generated instruction-tuning data, proposed benchmark are uploaded to the\n",
      "anonymized GitHub repository: LLaV A-Annonymous/LLaV A.\n",
      "1. Source Code: link\n",
      "2. README: link\n",
      "3. Instructions to launch the demo: link\n",
      "4. All prompts and few shot examples for querying GPT-4: link\n",
      "5. LLaV A-Instruct-158K: link\n",
      "6. LLaV A-Bench: COCO, In-The-Wild\n",
      "7. Model checkpoints. The size of the model checkpoints after compression is 25GB, which\n",
      "exceeds the 5GB limit of GitHub LFS (Large File Storage). We’ll release the checkpoint to\n",
      "the public, or upon request with reviewers for this submission.\n",
      "E Data\n",
      "Instructions for brief image description.The list of instructions used to briefly describe the image\n",
      "content are shown in Table 11. They present the same meaning with natural language variance.\n",
      "• \"Describe the image concisely.\"\n",
      "• \"Provide a brief description of the given image.\"\n",
      "• \"Offer a succinct explanation of the picture presented.\"\n",
      "• \"Summarize the visual content of the image.\"\n",
      "• \"Give a short and clear explanation of the subsequent image.\"\n",
      "• \"Share a concise interpretation of the image provided.\"\n",
      "• \"Present a compact description of the photo’s key features.\"\n",
      "• \"Relay a brief, clear account of the picture shown.\"\n",
      "• \"Render a clear and concise summary of the photo.\"\n",
      "• \"Write a terse but informative summary of the picture.\"\n",
      "• \"Create a compact narrative representing the image presented.\"\n",
      "Table 11: The list of instructions for brief image description.\n",
      "Instructions for detailed image description.The list of instructions used to describe the image\n",
      "content in detail are shown in Table 12. They present the same meaning with natural language\n",
      "variance.\n",
      "CC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and\n",
      "count the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller\n",
      "than 3, as they are usually rare combinations concept and attributes that has already been covered\n",
      "20 [{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', '_id': '9c1b515339ff4f05bf5eb8d0045d13f0', '_collection_name': 'demo_collection'}]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=1,\n",
    "    filter=models.Filter(\n",
    "        must=[\n",
    "            models.FieldCondition(\n",
    "                key=\"metadata.producer\",  # Note the \"metadata.\" prefix!\n",
    "                match=models.MatchValue(\n",
    "                    value=\"pdfTeX-1.40.25\"\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', '_id': '9c1b515339ff4f05bf5eb8d0045d13f0', '_collection_name': 'demo_collection'}, page_content='C Training Details\\nWe pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a\\nbatch size of 128, and fine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a\\nlearning rate of 2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no\\nweight decay and a cosine learning rate with a warmup ratio of 3%. During finetuning, FSDP (Full\\nShard Data Parallel) and gradient checkpointing is used to save GPU memory, and offloading is not\\nused. BF16 and TF32 are enabled to achieve a balance between speed and precision.\\nWe train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning\\non Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\\nD Assets\\nOur source code, generated instruction-tuning data, proposed benchmark are uploaded to the\\nanonymized GitHub repository: LLaV A-Annonymous/LLaV A.\\n1. Source Code: link\\n2. README: link\\n3. Instructions to launch the demo: link\\n4. All prompts and few shot examples for querying GPT-4: link\\n5. LLaV A-Instruct-158K: link\\n6. LLaV A-Bench: COCO, In-The-Wild\\n7. Model checkpoints. The size of the model checkpoints after compression is 25GB, which\\nexceeds the 5GB limit of GitHub LFS (Large File Storage). We’ll release the checkpoint to\\nthe public, or upon request with reviewers for this submission.\\nE Data\\nInstructions for brief image description.The list of instructions used to briefly describe the image\\ncontent are shown in Table 11. They present the same meaning with natural language variance.\\n• \"Describe the image concisely.\"\\n• \"Provide a brief description of the given image.\"\\n• \"Offer a succinct explanation of the picture presented.\"\\n• \"Summarize the visual content of the image.\"\\n• \"Give a short and clear explanation of the subsequent image.\"\\n• \"Share a concise interpretation of the image provided.\"\\n• \"Present a compact description of the photo’s key features.\"\\n• \"Relay a brief, clear account of the picture shown.\"\\n• \"Render a clear and concise summary of the photo.\"\\n• \"Write a terse but informative summary of the picture.\"\\n• \"Create a compact narrative representing the image presented.\"\\nTable 11: The list of instructions for brief image description.\\nInstructions for detailed image description.The list of instructions used to describe the image\\ncontent in detail are shown in Table 12. They present the same meaning with natural language\\nvariance.\\nCC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and\\ncount the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller\\nthan 3, as they are usually rare combinations concept and attributes that has already been covered\\n20')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query by turning into a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', '_id': '9c1b515339ff4f05bf5eb8d0045d13f0', '_collection_name': 'demo_collection'}, page_content='C Training Details\\nWe pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a\\nbatch size of 128, and fine-tune on the proposed LLaV A-Instruct-158K dataset for 3 epochs, with a\\nlearning rate of 2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no\\nweight decay and a cosine learning rate with a warmup ratio of 3%. During finetuning, FSDP (Full\\nShard Data Parallel) and gradient checkpointing is used to save GPU memory, and offloading is not\\nused. BF16 and TF32 are enabled to achieve a balance between speed and precision.\\nWe train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning\\non Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.\\nD Assets\\nOur source code, generated instruction-tuning data, proposed benchmark are uploaded to the\\nanonymized GitHub repository: LLaV A-Annonymous/LLaV A.\\n1. Source Code: link\\n2. README: link\\n3. Instructions to launch the demo: link\\n4. All prompts and few shot examples for querying GPT-4: link\\n5. LLaV A-Instruct-158K: link\\n6. LLaV A-Bench: COCO, In-The-Wild\\n7. Model checkpoints. The size of the model checkpoints after compression is 25GB, which\\nexceeds the 5GB limit of GitHub LFS (Large File Storage). We’ll release the checkpoint to\\nthe public, or upon request with reviewers for this submission.\\nE Data\\nInstructions for brief image description.The list of instructions used to briefly describe the image\\ncontent are shown in Table 11. They present the same meaning with natural language variance.\\n• \"Describe the image concisely.\"\\n• \"Provide a brief description of the given image.\"\\n• \"Offer a succinct explanation of the picture presented.\"\\n• \"Summarize the visual content of the image.\"\\n• \"Give a short and clear explanation of the subsequent image.\"\\n• \"Share a concise interpretation of the image provided.\"\\n• \"Present a compact description of the photo’s key features.\"\\n• \"Relay a brief, clear account of the picture shown.\"\\n• \"Render a clear and concise summary of the photo.\"\\n• \"Write a terse but informative summary of the picture.\"\\n• \"Create a compact narrative representing the image presented.\"\\nTable 11: The list of instructions for brief image description.\\nInstructions for detailed image description.The list of instructions used to describe the image\\ncontent in detail are shown in Table 12. They present the same meaning with natural language\\nvariance.\\nCC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and\\ncount the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller\\nthan 3, as they are usually rare combinations concept and attributes that has already been covered\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', '_id': '292c7d5cdc4749d3820b6c20f2a4c90e', '_collection_name': 'demo_collection'}, page_content='Method Subject Context Modality Grade AverageNAT SOC LAN TXT IMG NO G1-6 G7-12\\nRepresentative & SoTA methods with numbers reported in the literature\\nHuman [34] 90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40\\nGPT-3.5 [34] 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97\\nGPT-3.5 w/ CoT [34] 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17\\nLLaMA-Adapter [59] 84.37 88.30 84.36 83.72 80.32 86.90 85.83 84.05 85.19\\nMM-CoTBase[61] 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91\\nMM-CoTLarge[61] 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68\\nResults with our own experiment runs\\nGPT-4† 84.06 73.45 87.36 81.87 70.75 90.73 84.69 79.10 82.69\\nLLaV A 90.36 95.95 88.00 89.49 88.00 90.66 90.93 90.90 90.92\\nLLaV A+GPT-4† (complement) 90.36 95.50 88.55 89.05 87.80 91.08 92.22 88.73 90.97\\nLLaV A+GPT-4† (judge) 91.56 96.74 91.09 90.62 88.99 93.52 92.73 92.16 92.53\\nTable 7: Accuracy (%) on Science QA dataset. Question categories: NAT = natural science, SOC =\\nsocial science, LAN = language science, TXT = text context, IMG = image context, NO = no context,\\nG1-6 = grades 1-6, G7-12 = grades 7-12. †Text-only GPT-4, our eval. Our novel model ensembling\\nwith the text-only GPT-4 consistently improves the model’s performance under all categories, setting\\nthe new SoTA performance.\\nthis is the first time that GPT-4 is used for model ensembling. We hope this finding can encourage\\nfuture research to explore more effective methods to leverage LLMs for model ensembling.\\nVisual features Before Last\\nBest variant 90.92 89.96 (-0.96)\\nPredict answer first - 89.77 (-1.15)\\nTraining from scratch85.81(-5.11) -\\n7B model size 89.84(-1.08) -\\nTable 8: Design choice ablations (%). The differ-\\nence with the best variant is reported in red text.\\nAblations. We ablate several design choices\\non ScienceQA in Table 8. (i) Visual features.\\nWe tried using the last layer feature from CLIP\\nvision encoder, which yields 89.96% and is\\n0.96% lower than the feature before the last\\nlayer. We hypothesize that this is because\\nCLIP’s last layer features may focus more on\\nglobal and abstract image properties compared\\nto the layer before it, which can focus more on\\nlocalized properties that are useful for under-\\nstanding specific image details. (ii) Chain-of-thought. To decide the order between the answer\\nand reasoning process in the model prediction, we run both variants and observe that answer-first\\nreports the best number 89.77% accuracy in 12 epochs, while reasoning-first can quickly reach\\n89.77% accuracy in 6 epochs, but no further improvement with more training. Training the model\\nfor 24 epochs does not improve the performance. We conclude that CoT-like reasoning-first strategy\\ncan largely improve convergence, but contributes relatively little to the final performance. (iii)\\nPre-training. We skip pre-training and directly train on Science QA from scratch – performance drops\\nto 85.81% accuracy. The 5.11% absolute degradation indicates the importance of our pre-training\\nstage, in aligning multimodal features while preserving the vast pre-trained knowledge. (iv) Model\\nsize. We keep all configurations the same as our best 13B model, and train a 7B model. This yields\\n89.84% accuracy, which is 1.08% lower than 90.92%, demonstrating the importance of model scale.\\n6 Conclusion\\nThis paper demonstrated the effectiveness of visual instruction tuning. We presented an automatic\\npipeline to create language-image instruction-following data, based on which we train LLaV A, a\\nmultimodal model to follow human intent to complete visual tasks. It achieves the new SoTA\\naccuracy when fine-tuned on ScienceQA, and excellent visual chat capabilities when fine-tuned\\non multimodal chat data. Besides, we present the first benchmark to study multimodal instruction-\\nfollowing capability. This paper is an initial step in visual instruction tuning, and mainly focuses on\\nreal-life tasks. For more quantitative results of LLaV A on academic benchmarks, please refer to the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', '_id': '9617fe07f12741adb7108641bce6bc3a', '_collection_name': 'demo_collection'}, page_content='the text-only GPT-4, which cannot process images, improves the overall performance of the model\\non questions that have an image as context. This is because some of these questions do not actually\\nrequire the image context for a correct answer. The GPT-4 judge can identify such cases and correct\\nsome of the errors that LLaV A makes. See the example in Appendix. To the best of our knowledge,\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 13, 'page_label': '14', '_id': '727f32fcd09749ed8e4494e039f0d6b4', '_collection_name': 'demo_collection'}, page_content='A Broader Impact\\nThe broader impact of LLaV A, a general-purpose visual assistant, has potential benefits and risks\\nassociated with its deployment and release. Some considerations are unique to LLaV A due to its\\nvisual nature, while others share similarities with existing instruction-following LLMs (e.g., Alpaca,\\nVicuna, etc.). As LLaV A is built upon LLaMA, Vicuna, and CLIP, it inherits some of the issues\\nassociated with LLMs and vision encoders. In the following, we outline both the risks and mitigation\\nstrategies in place for the release of this model.\\nMalicious input. To minimize potential misuse and harmful consequences, we employ two pre-\\ncautionary measures for LLaV A: (1)OpenAI Filter APIfor user input text to prevent harmful or\\ninappropriate text instructions from being processed by the model, and (2) NSFW Filterfor uploaded\\nuser images to detect and block Not Safe For Work (NSFW) content or any other potentially harmful\\nimage inputs.\\nHallucination. Similar to LLMs, LLaV A might generate outputs that aren’t grounded in facts\\nor input data. This raises concerns about inferences made, especially in critical applications ( e.g.,\\nmedical).\\nBiases. Bias can be transferred from the base models to LLaV A, both from the vision encoder\\n(CLIP) and the language decoder (LLaMA/Vicuna). This may lead to biased outcomes or unfair\\nrepresentations of diverse content.\\nEnergy consumption. Though energy consumption is not a primary concern for LLaV A due to\\na smaller pretraining dataset (see details in Sec. C), it may become a concern when scaling up the\\npretraining dataset or increasing the model size, e.g., to a larger LLaMA version like the 65B model.\\nEvaluation complexities. Assessing the performance of LLaV A is challenging as it involves both\\nlanguage and visual tasks. Our evaluation benchmark covers several aspects, including accuracy,\\nconcept coverage, reasoning ability, and creativity. However, additional aspects need consideration,\\nsuch as the degree of visual content hallucination and fine-grained understanding of visual content.\\nWhile text-only GPT-4 based multimodal evaluation is consistent and accurate in our study, its\\nrobustness in different situations and capability to evaluate other unexplored aspects are subjects for\\nfuture work.\\nDespite these risks, we believe that the benefits of releasing LLaV A to the research community\\noutweigh the potential harm. It allows for ongoing investigation and improvement of the model and\\nengages the community in developing better mitigation strategies to address these concerns. Moreover,\\nthe release of LLaV A can spur the development of new applications and research directions, ultimately\\ncontributing to the progress and responsible deployment of foundation models in vision-language\\ntasks.\\nB More Results\\nWe present more qualitative results of LLaV A to analyze its emergent behaviors and observed\\nweaknesses. For more quantitative results of LLaV A on academic benchmarks, please refer to the\\nimproved baselines with visual instruction tuning [32]. In Table 9, LLaV A demonstrates a similar\\nbehavior as GPT-4 in another example from its paper. Similar to the GPT-4 live demo by OpenAI,\\nLLaV A is capable of generating the HTML/JS/CSS code for an interactive joke website based on\\na simplified user input sketch in Fig. 2, despite a minor error. As shown in Fig. 3, LLaV A can\\nfollow user’s instructions in a conversational style and provide detailed responses or creative writings.\\nFurthermore, LLaV A is able to relate the visual content to the textual knowledge from the pretrained\\nLLM, as demonstrated in Fig. 4 and Fig. 5.\\nOne interesting emergent behavior of LLaV A is that it is able to understand visual contents that\\nare not covered in the training. For example, in Fig. 6, it is able to recognize Elon Musk both in a\\nheadshot and in a humorous meme where he is dressed as a doge, even though Elon Musk never'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-12-14T01:02:53+00:00', 'author': '', 'keywords': '', 'moddate': '2023-12-14T01:02:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'data/visual_instruction_tunning.pdf', 'total_pages': 25, 'page': 18, 'page_label': '19', '_id': 'c520f0359bb6446284c74fc908958386', '_collection_name': 'demo_collection'}, page_content='The name of the man in the photo is Elon Musk.\\nWhat is the name of the man in the photo?\\nThe name of the man in the photo is Elon Musk.\\nWhat is the name of the man in the photo?\\nStart a new conversation, and the history is cleared.\\nUser\\nUser\\nLLaVA\\nLLaVA\\nFigure 6: An interesting emergent behavior of LLaV A is its ability to recognize Elon Musk both in a\\nheadshot and in a humorous meme where he is dressed as a doge. This implies that the pre-trained\\nCLIP vision encoder may have seen images of Elon Musk. However, it is still surprising because Elon\\nMusk never appears in the training data for either the visual feature alignment or visual instruction\\ntuning stages of LLaV A, which indicates that the base language model generalizes to unseen visual\\nconcepts.\\nQuestion:\\nWhich material is this rocking chair made of?\\nOptions: (A) wood (B) silk\\nLLaV A answer:\\nLECTURE: A material is a type of matter.\\nWood, glass, metal, and plastic are common materials.\\nSome objects are made of just one material.\\nMost nails are made of metal.\\nOther objects are made of more than one material.\\nThis hammer is made of metal and wood.\\nSOLUTION: Look at the picture of the rocking chair. The rocking chair is made of two different\\nmaterials. The legs are made of wood, and the back and seat are made of silk.\\nThe answer is B.\\nGPT-4 (text-only) answer:\\nThe answer is A. BECAUSE: Rocking chairs are typically made of sturdy materials that can support\\nthe weight of a person and withstand the rocking motion. Wood is a common material used for\\nmaking rocking chairs due to its strength, durability, and natural appearance.\\nGPT-4 (text-only) judge:\\nSince there is no context or image to refer to, I cannot independently analyze the rocking chair in\\nquestion. However, I can try to analyze the reasoning given by the assistants.\\nAssistant 1, who claims to have seen the image, mentions that the legs of the rocking chair are\\nmade of wood, and the back and seat are made of silk, implying that it’s a combination of two\\nmaterials. However, this response seems less plausible, as silk is not a common material used for\\nthe construction of rocking chairs due to issues with stability and durability. Typically, one would\\nexpect the frame, legs, and seat or back to be made of wood, with cushions and padding made of\\nfabrics, such as cotton or polyester.\\nAssistant 2 argues that rocking chairs are typically made of sturdy materials, and wood is a common\\nmaterial used for making rocking chairs due to its strength, durability, and natural appearance. This\\nreasoning stands true and is based on common knowledge.\\nGiven the information available, the most plausible answer is that the rocking chair is made of wood,\\nas suggested by Assistant 1. The answer is A.\\nTable 10: One example on how the text-only GPT-4 acts as a judge to ensemble the predictions from\\nLLaV A and a text-only GPT-4, and gives a correct final answer.\\n19')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
